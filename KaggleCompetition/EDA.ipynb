{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING ONLY RAW DATA AND NO FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def test_model_out_of_the_box(model, model_name):\n",
    "    model_all_data = copy.deepcopy(model)\n",
    "\n",
    "    #Load in data\n",
    "    df = pd.read_csv('./data/raw/train_final.csv', index_col = 0)\n",
    "    df_test = pd.read_csv('./data/raw/test_final.csv', index_col=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Y', axis=1), df['Y'], test_size=0.2, random_state=42) # Create split for dev stuff\n",
    "    x = df.drop('Y', axis=1)\n",
    "    y = df['Y']\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        model_all_data.fit(x, y, verbose=False)  \n",
    "    except:\n",
    "        model.fit(X_train, y_train)\n",
    "        model_all_data.fit(x, y)  \n",
    "\n",
    "    print('Using only train data AUC ROC score:', metrics.roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])) \n",
    "\n",
    "    # Spit Out\n",
    "    df_test['Y'] = model_all_data.predict_proba(df_test)[:,1]\n",
    "    df_test['Y'].to_csv(f'./preds/{model_name}_out_of_the_box.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Out of the box - 0.87046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only train data AUC ROC score: 0.874332337973672\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Get an idea of how out of the box submissions will look\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "test_model_out_of_the_box(model_xgb, model_name='xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Out of the box - 0.89352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only train data AUC ROC score: 0.9237498422845607\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "model_cb = cb.CatBoostClassifier()\n",
    "test_model_out_of_the_box(model_cb, model_name='cb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm Out of the box - 0.86941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jackson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Jackson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only train data AUC ROC score: 0.8872650039954578\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "test_model_out_of_the_box(lgbm.LGBMClassifier(), model_name='lgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Out of the Box - 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only train data AUC ROC score: 0.808386255625184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "test_model_out_of_the_box(RandomForestClassifier(), model_name='rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Re-Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_categorical(df):\n",
    "    cat = []\n",
    "    const = []\n",
    "    contin = []\n",
    "    for c in df.columns:\n",
    "        x = len(df[c].value_counts())\n",
    "        if x == 1:\n",
    "            const.append(c)\n",
    "            #print(f'Column {c} is likely CONSTANT')\n",
    "        elif x < 10:\n",
    "            #print(f'Column {c} is likely categorical w/ {x} categories')\n",
    "            cat.append((c, x))\n",
    "        else:\n",
    "            #print(f'Columns {c} is likely continuous... Has {x} unique values')\n",
    "            contin.append((c, x))\n",
    "\n",
    "    return np.array(cat), const, contin\n",
    "\n",
    "def convert_categorical(df, info, onehot=False):\n",
    "    cache = {}\n",
    "    for c, n in info:\n",
    "        col_cache = {}\n",
    "        u = np.sort(df[c].unique())\n",
    "        arr = np.arange(len(u))\n",
    "        f = lambda x: arr[np.where(u==x)[0][0]] if len(np.where(u==x)[0]) > 0 else x\n",
    "        df[c] = df[c].map(f)\n",
    "        col_cache['f'] = f\n",
    "        col_cache['arr'] = arr\n",
    "        col_cache['unique'] = u\n",
    "\n",
    "        if onehot:\n",
    "            pass #TODO convert the now categorized column into onehot representations\n",
    "            #Must store the one hotters for consistency\n",
    "\n",
    "        cache[c] = col_cache\n",
    "\n",
    "    return df, cache\n",
    "\n",
    "def train_preprocess(df, onehot=False):\n",
    "    categ, const, contin = find_categorical(df)\n",
    "    df = df.drop(const[1:], axis=1) # Drop all constants except 1\n",
    "    df, categ_cache = convert_categorical(df, categ, onehot)\n",
    "    cache = {\n",
    "        'categ': categ_cache,\n",
    "        'const': const\n",
    "    }\n",
    "    return df, cache\n",
    "\n",
    "def test_preprocess(df, cache):\n",
    "\n",
    "    # Drop all columns except 1 from cache['const']\n",
    "    df = df.drop(cache['const'][1:], axis=1)\n",
    "    # Convert all categorical from cache['categ']\n",
    "    for c in cache['categ']:\n",
    "        col_cache = cache['categ'][c]\n",
    "        arr = col_cache['arr']\n",
    "        u = col_cache['unique']\n",
    "        df[c] = df[c].map(col_cache['f'])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_df = pd.read_csv('./data/raw/train_final.csv', index_col=0)\n",
    "test_df = pd.read_csv('./data/raw/test_final.csv', index_col=0)\n",
    "\n",
    "x = train_df.drop('Y', axis=1)\n",
    "y = train_df['Y']\n",
    "pp_train, cache = train_preprocess(x.copy())\n",
    "pp_test = test_preprocess(test_df.copy(), cache)\n",
    "x_train, x_test, y_train, y_test = train_test_split(pp_train, y, test_size=0.2, random_state=42) # Create split for dev stuff\"\"\"\n",
    "\n",
    "df = pd.read_csv('./data/raw/train_final.csv', index_col = 0)\n",
    "x = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "test_df = pd.read_csv('./data/raw/test_final.csv', index_col=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) # Create split for dev stuff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST\n",
    "- Hyperparameter Tuned\n",
    "- Feature Engineered\n",
    "- Hyperparameter Tuned + Feature Engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineered XGBoost AUC score:  0.874332337973672\n"
     ]
    }
   ],
   "source": [
    "# ============== Feature Engineering ==================== <- Failure\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "print('Feature Engineered XGBoost AUC score: ', metrics.roc_auc_score(y_test, model.predict_proba(x_test)[:,1]))\n",
    "\n",
    "model = xgb.XGBClassifier().fit(pp_train, y)\n",
    "pp_test['Y'] = model.predict_proba(pp_test)[:,1]\n",
    "pp_test['Y'].to_csv('./preds/xgb_feature_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGBoost AUC score prediction:  0.9226563485721495\n"
     ]
    }
   ],
   "source": [
    "# ============== Hyperparameter Tuning ======================\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'eta': 0.04,\n",
    "    'gamma':0,\n",
    "    'max_depth':5,\n",
    "    'min_child_weight':1\n",
    "}\n",
    "model = xgb.XGBClassifier()\n",
    "model.set_params(**params)\n",
    "model.fit(x_train, y_train)\n",
    "print('Tuned XGBoost AUC score prediction: ', metrics.roc_auc_score(y_test, model.predict_proba(x_test)[:,1]))\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.set_params(**params)\n",
    "model.fit(x, y)\n",
    "xg_preds = pd.DataFrame(model.predict_proba(test_df)[:,1], index=test_df.index, columns=['Y'])\n",
    "xg_preds.to_csv('./preds/xgb_tuned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = params = {\n",
    "            'eta': 0.02,\n",
    "            'max_depth':4,\n",
    "            'n_estimators':1000\n",
    "        }\n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Scores==================\n",
      "\tm0:  0.9306317539484621\n",
      "\tm1:  0.9052008863246791\n",
      "\tm2:  0.9108016067062522\n",
      "\tm3:  0.9176688063703583\n",
      "\tm4:  0.9020912938082912\n",
      "\tm5:  0.9058070989589866\n",
      "\tm6:  0.9037531476078181\n",
      "\tm7:  0.9089694486505745\n",
      "\tm8:  0.9040442741592167\n",
      "\tm9:  0.8552892561983472\n",
      "Predicted AUC score: 0.9044257572732987\n",
      "Best Score: 0\n",
      "!!!!!!!!!!!!!!!Found new best parameters!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# ============== Hyperparameter Tuning ======================\n",
    "n_folds = 10\n",
    "print('=================Scores==================')\n",
    "tot_score = 0\n",
    "for i in range(n_folds):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # Create split for dev stuff\n",
    "    params = {\n",
    "            'eta': 0.01,\n",
    "            'max_depth':5,\n",
    "            'n_estimators':1200\n",
    "        }\n",
    "    model = cb.CatBoostClassifier()\n",
    "    model.set_params(**params)\n",
    "    model.fit(x_train, y_train, verbose=False)\n",
    "    score = metrics.roc_auc_score(y_test, model.predict_proba(x_test)[:,1])\n",
    "    tot_score += score\n",
    "    print(f'\\tm{i}: ', score)\n",
    "print(f'Predicted AUC score: {tot_score/n_folds}\\nBest Previous Score: {best_score}')\n",
    "if best_score < tot_score/n_folds:\n",
    "    print('!!!!!!!!!!!!!!!Found new best parameters!!!!!!!!')\n",
    "    best_score = tot_score/n_folds\n",
    "    best_params = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostClassifier()\n",
    "model.set_params(**best_params)\n",
    "model.fit(x, y, verbose=False)\n",
    "cb_preds = pd.DataFrame(model.predict_proba(test_df)[:,1], index=test_df.index, columns=['Y'])\n",
    "cb_preds.to_csv('./preds/cb_tuned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/raw/train_final.csv', index_col = 0)\n",
    "x = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "test_df = pd.read_csv('./data/raw/test_final.csv', index_col=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) # Create split for dev stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CNN Using Torch ==================\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        # Probably want to include parameters here for defining the structure of the neural net\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def train(self, train_df):\n",
    "        pass\n",
    "\n",
    "    def predict(self, df):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, df):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "- Stacked model of each best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Ensembling =================\n",
    "\n",
    "# Stacking\n",
    "# Create dataframe of predictions from xgboost, catboost, and neural net\n",
    "# Train logistic regression, neural net, xgboost, and catboost on \n",
    "n_folds = 10\n",
    "tot_score = 0\n",
    "for i in range(n_folds):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # Create split for dev stuff\n",
    "    #Train best_params catboost\n",
    "    #Train best_params xgboost\n",
    "    ens_df = xg_preds.join(cb_preds, lsuffix='xg', rsuffix='cat')\n",
    "\n",
    "    #Train catboost on stacked df\n",
    "\n",
    "    #Test ensembled model on test data produce AUC score\n",
    "\n",
    "tot_score /= n_folds\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
