{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 0 - Jhanvi\n",
    "\n",
    "As we have covered in class, we are training a logistic regression model to predict if\n",
    "someone will click on an advertisement. Consider the logistic regression model with 3 features and\n",
    "weights w = [1, −30, 3].\n",
    "\n",
    "For the dataset with features\n",
    "x1=[20,0,0], y1=1\n",
    "x2=[23,1,1], y2=0,\n",
    "•Compute the probabilities that the logistic regression assigns to these two customers clicking\n",
    "on the advertisement (i.e. y=1)\n",
    "•Compute the cross entropy loss of this logistic regression.\n",
    "•Design a decision stump (a decision tree of depth 1) that splits on the first feature. What is\n",
    "the Gini impurity of the root? What is the Gini impurity after the best split that you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logisitic regression assigned the following probabilities to the customers: \n",
      "[1.         0.01798621]\n",
      "The cross entropy loss is: \n",
      "2.0611536942919273e-09\n",
      "\n",
      "To reflect, the model predicts that the first customer will click on an advertisement with very high probability and is correct. As such, the loss is very small.\n",
      "x[0][0]: 20\n",
      "x[1][0]: 23\n",
      "The gini impurity for the left and right buckets at a split of 21.5 is (0.0, 0.0)\n",
      "\n",
      "The impurity calculated is 0, which is the best possible Gini impurity for a bucket, which indicates that all the elements in that bucket are of the same class.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "w = ([1, -30, 3])\n",
    "x = ([20, 0, 0], [23, 1, 1])\n",
    "y = ([1, 0])\n",
    "\n",
    "def logistic_reg(w, x):\n",
    "    col_x = np.transpose(x)\n",
    "    scores = np.dot(w, col_x)\n",
    "\n",
    "    sigmoid_1 = float(1 / (1 + math.exp(-1 * scores[0])))\n",
    "    sigmoid_2 = float(1 / (1 + math.exp(-1 * scores[1])))\n",
    "\n",
    "    sigmoid_matrix = np.array([sigmoid_1, sigmoid_2])\n",
    "\n",
    "    return sigmoid_matrix\n",
    "\n",
    "def cross_entropy(y, sigmoid_matrix):\n",
    "    probability = (y[0] * sigmoid_matrix[0]) + (y[1] * sigmoid_matrix[1])\n",
    "    return -1 * math.log(probability)\n",
    "\n",
    "sigmoid_matrix = logistic_reg(w, x)\n",
    "loss = cross_entropy(np.transpose(y), sigmoid_matrix)\n",
    "\n",
    "print(\"The logisitic regression assigned the following probabilities to the customers: \")\n",
    "print(sigmoid_matrix)\n",
    "print(\"The cross entropy loss is: \")\n",
    "print(loss)\n",
    "\n",
    "print(\"\\nTo reflect, the model predicts that the first customer will click on an advertisement with very high probability and is correct. As such, the loss is very small.\")\n",
    "\n",
    "def decision_tree(x):\n",
    "    col_x = np.transpose(x)\n",
    "    split = (x[0][0] + x[1][0]) / 2\n",
    "    print(\"x[0][0]: \" + str(x[0][0]))\n",
    "    print(\"x[1][0]: \" + str(x[1][0]))\n",
    "\n",
    "    if x[0][0] > split:\n",
    "        right = 0\n",
    "    else :\n",
    "        left = 0\n",
    "\n",
    "    if x[1][0] > split:\n",
    "        right = 1\n",
    "    else :\n",
    "        left = 1\n",
    "\n",
    "    return left, right, split\n",
    "\n",
    "def gini_impurity(l_bucket, r_bucket):\n",
    "    num_0_left, num_1_right, num_0_right, num_1_left = 0, 0, 0, 0\n",
    "\n",
    "    if l_bucket == 0:\n",
    "        num_0_left += 1\n",
    "    if r_bucket == 1:\n",
    "        num_1_right += 1\n",
    "    bucket_total = 1\n",
    "\n",
    "    gini_left = ((num_0_left/bucket_total) * (1 - num_0_left/bucket_total)) + ((num_1_left/bucket_total) * (1 - num_1_left/bucket_total))\n",
    "    gini_right = ((num_0_right/bucket_total) * (1 - num_0_right/bucket_total)) + ((num_1_right/bucket_total) * (1 - num_1_right/bucket_total))\n",
    "\n",
    "    return gini_left, gini_right\n",
    "\n",
    "left, right, split = decision_tree(x)\n",
    "gini = gini_impurity(left, right)\n",
    "print(\"The gini impurity for the left and right buckets at a split of {} is {}\".format(split, gini))\n",
    "print(\"\\nThe impurity calculated is 0, which is the best possible Gini impurity for a bucket, which indicates that all the elements in that bucket are of the same class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 1: Logistic Regression and CIFAR-10. - Jhanvi\n",
    "In this problem you will explore the dataset\n",
    "CIFAR-10, and you will use multinomial (multi-label) Logistic Regression to try to classify it. You\n",
    "will also explore visualizing the solution.\n",
    "\n",
    "(Optional) You can read about the CIFAR-10 and CIFAR-100 datasets here: https://www.\n",
    "cs.toronto.edu/~kriz/cifar.html.\n",
    "•(Optional) OpenML curates a number of data sets. You will use a subset of CIFAR-10\n",
    "provided by them. Read here for a description: https://www.openml.org/d/40926.\n",
    "•Use the fetch openml command from sklearn.datasets to import the CIFAR-10-Small\n",
    "data set.\n",
    "•Figure out how to display some of the images in this data set, and display a couple. While\n",
    "not high resolution, these should be recognizable if you are doing it correctly.\n",
    "•There are 20,000 data points. Do a train-test split on 3/4 - 1/4.\n",
    "•You will run multi-class logistic regression on these using the cross entropy loss. You have to\n",
    "specify this specifically (multi class=’multinomial’). Use cross validation to see how good\n",
    "your accuracy can be. In this case, cross validate to find as good regularization coefficients\n",
    "as you can, for ℓ1 and ℓ2 regularization (called penalties), which are naturally supported in\n",
    "sklearn.linear model.LogisticRegression. I recommend you use the solver saga.\n",
    "•Report your training and test loss from above,\n",
    "•How sparse can you make your solutions without deteriorating your testing error too much?\n",
    "Here, we ask for a sparse solution that has test accuracy that is close to the best solution you\n",
    "found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 2: Multi-class Logistic Regression – Visualizing the Solution.  - Josh\n",
    "You will repeat\n",
    "the previous problem but for the MNIST dataset which you will find here: https://www.openml.\n",
    "org/d/554. MNIST is a dataset of handwritten digits, and is considered one of the easiest image\n",
    "recognition problems in computer vision. We will see here how well logistic regression does, as you\n",
    "did above on the CIFAR-10 subset. In addition, we will see that we can visualize the solution, and\n",
    "that in connection to this, sparsity can be useful.\n",
    "•Use the fetch openml command from sklearn.datasets to import the MNIST data set,\n",
    "•Choose a reasonable train-test split, and again run multi-class logistic regression on these\n",
    "using the cross entropy loss, as you did above. Try to optimize the hyperparameters.\n",
    "•Report your training and test loss from above,\n",
    "•Choose an ℓ1 regularizer (penalty), and see if you can get a sparse solution with almost as\n",
    "good accuracy.\n",
    "•Note that in Logistic Regression, the coefficients returned (i.e., the β’s) are the same dimen-\n",
    "sion as the data. Therefore we can pretend that the coefficients of the solution are an image\n",
    "of the same dimension, and plot it. Do this for the 10 sets of coefficients that correspond to\n",
    "the 10 classes. You should observe that, at least for the sparse solutions, these “kind of” look\n",
    "like the digits they are classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 3: Revisiting Logistic Regression and MNIST. - Josh\n",
    "Here we throw the kitchen sink of classical ML (i.e. pre-deep learning) on MNIST.\n",
    "•Use Random Forests to try to get the best possible test accuracy on MNIST. Use Cross\n",
    "Validation to find the best settings. How well can you do? You should use the accuracy\n",
    "metric to compare to logistic regression. What are the hyperparameters of your best model?\n",
    "•Use Gradient Boosting to do the same. Try your best to tune your hyper parameters. What\n",
    "are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 4: Revisiting Logistic Regression and CIFAR-10. - Jackson\n",
    "As before, we’ll throw the kitchen sink of classical ML (i.e. pre-deep learning) on CIFAR-10.  \n",
    "Keep in mind that CIFAR-10 is a few times larger.\n",
    "* What is the best accuracy you can get on the test data, by tuning Random Forests? \n",
    "    * What are the hyperparameters of your best model?\n",
    "* What is the best accuracy you can get on the test data, by tuning any model including Gradient boosting? \n",
    "    * What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 5: Getting Started with Pytorch. - Jackson\n",
    " * Install Pytorch.\n",
    " * Work through this tutorial to familiarize yourself with Pytorch basics: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    " * Work through this tutorial on MNIST starting from a Pytorch logistic regression and building to a CNN using torch.nn. Use a GPU (e.g. on Colab, through Google Cloud credits, Pa-perspace, or any other way). https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    " * Design the best CNN you can to get the best accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 6: CNNs for CIFAR-10. - Jackson\n",
    "* Build a CNN and optimize the accuracy for CIFAR-10. \n",
    "    * Try different number of layers and different architectures (depth and convolutional filter hyperparameters).\n",
    "* Is momentum and learning rate having a significant effect? \n",
    "    * Track the train and test loss across training epochs and plot them for different learning rates and momentum values.\n",
    "* Is the depth of the CNN having a significant effect on performance? \n",
    "    * Describe the hyperparameters of the best model you could train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}