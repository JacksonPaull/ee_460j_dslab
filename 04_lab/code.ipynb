{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 0 - Jhanvi\n",
    "\n",
    "As we have covered in class, we are training a logistic regression model to predict if\n",
    "someone will click on an advertisement. Consider the logistic regression model with 3 features and\n",
    "weights w = [1, −30, 3].\n",
    "\n",
    "For the dataset with features\n",
    "x1=[20,0,0], y1=1\n",
    "x2=[23,1,1], y2=0,\n",
    "•Compute the probabilities that the logistic regression assigns to these two customers clicking\n",
    "on the advertisement (i.e. y=1)\n",
    "•Compute the cross entropy loss of this logistic regression.\n",
    "•Design a decision stump (a decision tree of depth 1) that splits on the first feature. What is\n",
    "the Gini impurity of the root? What is the Gini impurity after the best split that you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logisitic regression assigned the following probabilities to the customers: \n",
      "[1.         0.01798621]\n",
      "The cross entropy loss is: \n",
      "2.0611536942919273e-09\n",
      "\n",
      "To reflect, the model predicts that the first customer will click on an advertisement with very high probability and is correct. As such, the loss is very small.\n",
      "x[0][0]: 20\n",
      "x[1][0]: 23\n",
      "The gini impurity for the left and right buckets at a split of 21.5 is (0.0, 0.0)\n",
      "\n",
      "The impurity calculated is 0, which is the best possible Gini impurity for a bucket, which indicates that all the elements in that bucket are of the same class.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "w = ([1, -30, 3])\n",
    "x = ([20, 0, 0], [23, 1, 1])\n",
    "y = ([1, 0])\n",
    "\n",
    "def logistic_reg(w, x):\n",
    "    col_x = np.transpose(x)\n",
    "    scores = np.dot(w, col_x)\n",
    "\n",
    "    sigmoid_1 = float(1 / (1 + math.exp(-1 * scores[0])))\n",
    "    sigmoid_2 = float(1 / (1 + math.exp(-1 * scores[1])))\n",
    "\n",
    "    sigmoid_matrix = np.array([sigmoid_1, sigmoid_2])\n",
    "\n",
    "    return sigmoid_matrix\n",
    "\n",
    "def cross_entropy(y, sigmoid_matrix):\n",
    "    probability = (y[0] * sigmoid_matrix[0]) + (y[1] * sigmoid_matrix[1])\n",
    "    return -1 * math.log(probability)\n",
    "\n",
    "sigmoid_matrix = logistic_reg(w, x)\n",
    "loss = cross_entropy(np.transpose(y), sigmoid_matrix)\n",
    "\n",
    "print(\"The logisitic regression assigned the following probabilities to the customers: \")\n",
    "print(sigmoid_matrix)\n",
    "print(\"The cross entropy loss is: \")\n",
    "print(loss)\n",
    "\n",
    "print(\"\\nTo reflect, the model predicts that the first customer will click on an advertisement with very high probability and is correct. As such, the loss is very small.\")\n",
    "\n",
    "def decision_tree(x):\n",
    "    col_x = np.transpose(x)\n",
    "    split = (x[0][0] + x[1][0]) / 2\n",
    "    print(\"x[0][0]: \" + str(x[0][0]))\n",
    "    print(\"x[1][0]: \" + str(x[1][0]))\n",
    "\n",
    "    if x[0][0] > split:\n",
    "        right = 0\n",
    "    else :\n",
    "        left = 0\n",
    "\n",
    "    if x[1][0] > split:\n",
    "        right = 1\n",
    "    else :\n",
    "        left = 1\n",
    "\n",
    "    return left, right, split\n",
    "\n",
    "def gini_impurity(l_bucket, r_bucket):\n",
    "    num_0_left, num_1_right, num_0_right, num_1_left = 0, 0, 0, 0\n",
    "\n",
    "    if l_bucket == 0:\n",
    "        num_0_left += 1\n",
    "    if r_bucket == 1:\n",
    "        num_1_right += 1\n",
    "    bucket_total = 1\n",
    "\n",
    "    gini_left = ((num_0_left/bucket_total) * (1 - num_0_left/bucket_total)) + ((num_1_left/bucket_total) * (1 - num_1_left/bucket_total))\n",
    "    gini_right = ((num_0_right/bucket_total) * (1 - num_0_right/bucket_total)) + ((num_1_right/bucket_total) * (1 - num_1_right/bucket_total))\n",
    "\n",
    "    return gini_left, gini_right\n",
    "\n",
    "left, right, split = decision_tree(x)\n",
    "gini = gini_impurity(left, right)\n",
    "print(\"The gini impurity for the left and right buckets at a split of {} is {}\".format(split, gini))\n",
    "print(\"\\nThe impurity calculated is 0, which is the best possible Gini impurity for a bucket, which indicates that all the elements in that bucket are of the same class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 1: Logistic Regression and CIFAR-10. - Jhanvi\n",
    "In this problem you will explore the dataset\n",
    "CIFAR-10, and you will use multinomial (multi-label) Logistic Regression to try to classify it. You\n",
    "will also explore visualizing the solution.\n",
    "\n",
    "(Optional) You can read about the CIFAR-10 and CIFAR-100 datasets here: https://www.\n",
    "cs.toronto.edu/~kriz/cifar.html.\n",
    "•(Optional) OpenML curates a number of data sets. You will use a subset of CIFAR-10\n",
    "provided by them. Read here for a description: https://www.openml.org/d/40926.\n",
    "•Use the fetch openml command from sklearn.datasets to import the CIFAR-10-Small\n",
    "data set.\n",
    "•Figure out how to display some of the images in this data set, and display a couple. While\n",
    "not high resolution, these should be recognizable if you are doing it correctly.\n",
    "•There are 20,000 data points. Do a train-test split on 3/4 - 1/4.\n",
    "•You will run multi-class logistic regression on these using the cross entropy loss. You have to\n",
    "specify this specifically (multi class=’multinomial’). Use cross validation to see how good\n",
    "your accuracy can be. In this case, cross validate to find as good regularization coefficients\n",
    "as you can, for ℓ1 and ℓ2 regularization (called penalties), which are naturally supported in\n",
    "sklearn.linear model.LogisticRegression. I recommend you use the solver saga.\n",
    "•Report your training and test loss from above,\n",
    "•How sparse can you make your solutions without deteriorating your testing error too much?\n",
    "Here, we ask for a sparse solution that has test accuracy that is close to the best solution you\n",
    "found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data + 1665416577.0736032\n",
      "Splitting data + 1665416845.441797\n",
      "Skipped image creation + 1665416845.614722\n",
      "starting log reg + 1665416845.614774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 247 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 286 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 273 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 281 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 282 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.29057382\n",
      "Epoch 3, change: 0.17171912\n",
      "Epoch 4, change: 0.13320643\n",
      "Epoch 5, change: 0.11206841\n",
      "Epoch 6, change: 0.09193390\n",
      "Epoch 7, change: 0.08154919\n",
      "Epoch 8, change: 0.06753830\n",
      "Epoch 9, change: 0.06351508\n",
      "Epoch 10, change: 0.05537504\n",
      "Epoch 11, change: 0.05149556\n",
      "Epoch 12, change: 0.04640000\n",
      "Epoch 13, change: 0.04257978\n",
      "Epoch 14, change: 0.03888481\n",
      "Epoch 15, change: 0.03541193\n",
      "Epoch 16, change: 0.03351029\n",
      "Epoch 17, change: 0.03087188\n",
      "Epoch 18, change: 0.02911799\n",
      "Epoch 19, change: 0.02717832\n",
      "Epoch 20, change: 0.02556634\n",
      "Epoch 21, change: 0.02434719\n",
      "Epoch 22, change: 0.02380851\n",
      "Epoch 23, change: 0.02276080\n",
      "Epoch 24, change: 0.02208480\n",
      "Epoch 25, change: 0.02136580\n",
      "Epoch 26, change: 0.02038001\n",
      "Epoch 27, change: 0.02013560\n",
      "Epoch 28, change: 0.01932753\n",
      "Epoch 29, change: 0.01882016\n",
      "Epoch 30, change: 0.01846236\n",
      "Epoch 31, change: 0.01777304\n",
      "Epoch 32, change: 0.01740001\n",
      "Epoch 33, change: 0.01690220\n",
      "Epoch 34, change: 0.01638886\n",
      "Epoch 35, change: 0.01593212\n",
      "Epoch 36, change: 0.01558369\n",
      "Epoch 37, change: 0.01528505\n",
      "Epoch 38, change: 0.01482438\n",
      "Epoch 39, change: 0.01442248\n",
      "Epoch 40, change: 0.01429589\n",
      "Epoch 41, change: 0.01384172\n",
      "Epoch 42, change: 0.01358618\n",
      "Epoch 43, change: 0.01331906\n",
      "Epoch 44, change: 0.01303411\n",
      "Epoch 45, change: 0.01271214\n",
      "Epoch 46, change: 0.01243104\n",
      "Epoch 47, change: 0.01216667\n",
      "Epoch 48, change: 0.01187375\n",
      "Epoch 49, change: 0.01164771\n",
      "Epoch 50, change: 0.01152233\n",
      "Epoch 51, change: 0.01123786\n",
      "Epoch 52, change: 0.01098610\n",
      "Epoch 53, change: 0.01079597\n",
      "Epoch 54, change: 0.01062724\n",
      "Epoch 55, change: 0.01041563\n",
      "Epoch 56, change: 0.01028609\n",
      "Epoch 57, change: 0.01007046\n",
      "Epoch 58, change: 0.00987599\n",
      "Epoch 59, change: 0.00967889\n",
      "Epoch 60, change: 0.00951888\n",
      "Epoch 61, change: 0.00941198\n",
      "Epoch 62, change: 0.00920443\n",
      "Epoch 63, change: 0.00904468\n",
      "Epoch 64, change: 0.00890743\n",
      "Epoch 65, change: 0.00881525\n",
      "Epoch 66, change: 0.00861824\n",
      "Epoch 67, change: 0.00851701\n",
      "Epoch 68, change: 0.00834395\n",
      "Epoch 69, change: 0.00825372\n",
      "Epoch 70, change: 0.00818323\n",
      "Epoch 71, change: 0.00802892\n",
      "Epoch 72, change: 0.00798363\n",
      "Epoch 73, change: 0.00785965\n",
      "Epoch 74, change: 0.00787845\n",
      "Epoch 75, change: 0.00776511\n",
      "Epoch 76, change: 0.00763144\n",
      "Epoch 77, change: 0.00758589\n",
      "Epoch 78, change: 0.00751935\n",
      "Epoch 79, change: 0.00743789\n",
      "Epoch 80, change: 0.00737760\n",
      "Epoch 81, change: 0.00730086\n",
      "Epoch 82, change: 0.00720471\n",
      "Epoch 83, change: 0.00715812\n",
      "Epoch 84, change: 0.00708019\n",
      "Epoch 85, change: 0.00700781\n",
      "Epoch 86, change: 0.00689113\n",
      "Epoch 87, change: 0.00684577\n",
      "Epoch 88, change: 0.00679091\n",
      "Epoch 89, change: 0.00670619\n",
      "Epoch 90, change: 0.00663801\n",
      "Epoch 91, change: 0.00654492\n",
      "Epoch 92, change: 0.00649387\n",
      "Epoch 93, change: 0.00644720\n",
      "Epoch 94, change: 0.00632740\n",
      "Epoch 95, change: 0.00633230\n",
      "Epoch 96, change: 0.00627835\n",
      "Epoch 97, change: 0.00625269\n",
      "Epoch 98, change: 0.00619658\n",
      "Epoch 99, change: 0.00613009\n",
      "Epoch 100, change: 0.00608779\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.27959865\n",
      "Epoch 3, change: 0.22025206\n",
      "Epoch 4, change: 0.14609153\n",
      "Epoch 5, change: 0.11330281\n",
      "Epoch 6, change: 0.08973628\n",
      "Epoch 7, change: 0.08114136\n",
      "Epoch 8, change: 0.06911990\n",
      "Epoch 9, change: 0.06067740\n",
      "Epoch 10, change: 0.05317329\n",
      "Epoch 11, change: 0.04791224\n",
      "Epoch 12, change: 0.04343316\n",
      "Epoch 13, change: 0.03964011\n",
      "Epoch 14, change: 0.03657480\n",
      "Epoch 15, change: 0.03407345\n",
      "Epoch 16, change: 0.03174819\n",
      "Epoch 17, change: 0.03013018\n",
      "Epoch 18, change: 0.02867996\n",
      "Epoch 19, change: 0.02711314\n",
      "Epoch 20, change: 0.02551723\n",
      "Epoch 21, change: 0.02452239\n",
      "Epoch 22, change: 0.02352422\n",
      "Epoch 23, change: 0.02239535\n",
      "Epoch 24, change: 0.02148715\n",
      "Epoch 25, change: 0.02084410\n",
      "Epoch 26, change: 0.01984558\n",
      "Epoch 27, change: 0.01908710\n",
      "Epoch 28, change: 0.01823685\n",
      "Epoch 29, change: 0.01785470\n",
      "Epoch 30, change: 0.01698444\n",
      "Epoch 31, change: 0.01665361\n",
      "Epoch 32, change: 0.01604490\n",
      "Epoch 33, change: 0.01566356\n",
      "Epoch 34, change: 0.01539988\n",
      "Epoch 35, change: 0.01495247\n",
      "Epoch 36, change: 0.01463429\n",
      "Epoch 37, change: 0.01436847\n",
      "Epoch 38, change: 0.01398592\n",
      "Epoch 39, change: 0.01375058\n",
      "Epoch 40, change: 0.01340879\n",
      "Epoch 41, change: 0.01322288\n",
      "Epoch 42, change: 0.01288483\n",
      "Epoch 43, change: 0.01264190\n",
      "Epoch 44, change: 0.01244821\n",
      "Epoch 45, change: 0.01220488\n",
      "Epoch 46, change: 0.01188785\n",
      "Epoch 47, change: 0.01165343\n",
      "Epoch 48, change: 0.01151622\n",
      "Epoch 49, change: 0.01127134\n",
      "Epoch 50, change: 0.01098967\n",
      "Epoch 51, change: 0.01091088\n",
      "Epoch 52, change: 0.01066028\n",
      "Epoch 53, change: 0.01052394\n",
      "Epoch 54, change: 0.01029893\n",
      "Epoch 55, change: 0.01019432\n",
      "Epoch 56, change: 0.00989707\n",
      "Epoch 57, change: 0.00980226\n",
      "Epoch 58, change: 0.00965387\n",
      "Epoch 59, change: 0.00952207\n",
      "Epoch 60, change: 0.00927795\n",
      "Epoch 61, change: 0.00916961\n",
      "Epoch 62, change: 0.00904188\n",
      "Epoch 63, change: 0.00892259\n",
      "Epoch 64, change: 0.00875205\n",
      "Epoch 65, change: 0.00870341\n",
      "Epoch 66, change: 0.00857969\n",
      "Epoch 67, change: 0.00847963\n",
      "Epoch 68, change: 0.00842426\n",
      "Epoch 69, change: 0.00834843\n",
      "Epoch 70, change: 0.00825663\n",
      "Epoch 71, change: 0.00825558\n",
      "Epoch 72, change: 0.00806729\n",
      "Epoch 73, change: 0.00805701\n",
      "Epoch 74, change: 0.00790985\n",
      "Epoch 75, change: 0.00784175\n",
      "Epoch 76, change: 0.00782987\n",
      "Epoch 77, change: 0.00774546\n",
      "Epoch 78, change: 0.00765884\n",
      "Epoch 79, change: 0.00763216\n",
      "Epoch 80, change: 0.00752924\n",
      "Epoch 81, change: 0.00740519\n",
      "Epoch 82, change: 0.00737377\n",
      "Epoch 83, change: 0.00725068\n",
      "Epoch 84, change: 0.00722601\n",
      "Epoch 85, change: 0.00717503\n",
      "Epoch 86, change: 0.00709747\n",
      "Epoch 87, change: 0.00696884\n",
      "Epoch 88, change: 0.00693488\n",
      "Epoch 89, change: 0.00682262\n",
      "Epoch 90, change: 0.00675724\n",
      "Epoch 91, change: 0.00668566\n",
      "Epoch 92, change: 0.00668053\n",
      "Epoch 93, change: 0.00654239\n",
      "Epoch 94, change: 0.00652364\n",
      "Epoch 95, change: 0.00642622\n",
      "Epoch 96, change: 0.00641224\n",
      "Epoch 97, change: 0.00636838\n",
      "Epoch 98, change: 0.00627512\n",
      "Epoch 99, change: 0.00622462\n",
      "Epoch 100, change: 0.00619577\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.30488671\n",
      "Epoch 3, change: 0.17235108\n",
      "Epoch 4, change: 0.12274451\n",
      "Epoch 5, change: 0.10762437\n",
      "Epoch 6, change: 0.08838841\n",
      "Epoch 7, change: 0.07507343\n",
      "Epoch 8, change: 0.06847760\n",
      "Epoch 9, change: 0.05974727\n",
      "Epoch 10, change: 0.05426493\n",
      "Epoch 11, change: 0.04989756\n",
      "Epoch 12, change: 0.04560639\n",
      "Epoch 13, change: 0.04257770\n",
      "Epoch 14, change: 0.03997334\n",
      "Epoch 15, change: 0.03658548\n",
      "Epoch 16, change: 0.03418495\n",
      "Epoch 17, change: 0.03231377\n",
      "Epoch 18, change: 0.03028795\n",
      "Epoch 19, change: 0.02869024\n",
      "Epoch 20, change: 0.02751063\n",
      "Epoch 21, change: 0.02565885\n",
      "Epoch 22, change: 0.02481948\n",
      "Epoch 23, change: 0.02382565\n",
      "Epoch 24, change: 0.02277380\n",
      "Epoch 25, change: 0.02216347\n",
      "Epoch 26, change: 0.02152483\n",
      "Epoch 27, change: 0.02088435\n",
      "Epoch 28, change: 0.02021916\n",
      "Epoch 29, change: 0.01982066\n",
      "Epoch 30, change: 0.01930146\n",
      "Epoch 31, change: 0.01883641\n",
      "Epoch 32, change: 0.01845143\n",
      "Epoch 33, change: 0.01777075\n",
      "Epoch 34, change: 0.01736897\n",
      "Epoch 35, change: 0.01695680\n",
      "Epoch 36, change: 0.01674948\n",
      "Epoch 37, change: 0.01627587\n",
      "Epoch 38, change: 0.01598590\n",
      "Epoch 39, change: 0.01557001\n",
      "Epoch 40, change: 0.01525484\n",
      "Epoch 41, change: 0.01496085\n",
      "Epoch 42, change: 0.01464222\n",
      "Epoch 43, change: 0.01437258\n",
      "Epoch 44, change: 0.01408233\n",
      "Epoch 45, change: 0.01372775\n",
      "Epoch 46, change: 0.01360915\n",
      "Epoch 47, change: 0.01330329\n",
      "Epoch 48, change: 0.01304925\n",
      "Epoch 49, change: 0.01284019\n",
      "Epoch 50, change: 0.01258249\n",
      "Epoch 51, change: 0.01233426\n",
      "Epoch 52, change: 0.01211914\n",
      "Epoch 53, change: 0.01184361\n",
      "Epoch 54, change: 0.01167869\n",
      "Epoch 55, change: 0.01146184\n",
      "Epoch 56, change: 0.01121964\n",
      "Epoch 57, change: 0.01107481\n",
      "Epoch 58, change: 0.01088119\n",
      "Epoch 59, change: 0.01064345\n",
      "Epoch 60, change: 0.01055042\n",
      "Epoch 61, change: 0.01039421\n",
      "Epoch 62, change: 0.01040211\n",
      "Epoch 63, change: 0.01024100\n",
      "Epoch 64, change: 0.01009995\n",
      "Epoch 65, change: 0.00994923\n",
      "Epoch 66, change: 0.00987340\n",
      "Epoch 67, change: 0.00976713\n",
      "Epoch 68, change: 0.00959500\n",
      "Epoch 69, change: 0.00949310\n",
      "Epoch 70, change: 0.00945039\n",
      "Epoch 71, change: 0.00937056\n",
      "Epoch 72, change: 0.00918396\n",
      "Epoch 73, change: 0.00917880\n",
      "Epoch 74, change: 0.00906121\n",
      "Epoch 75, change: 0.00903505\n",
      "Epoch 76, change: 0.00888191\n",
      "Epoch 77, change: 0.00879601\n",
      "Epoch 78, change: 0.00873876\n",
      "Epoch 79, change: 0.00865684\n",
      "Epoch 80, change: 0.00858726\n",
      "Epoch 81, change: 0.00843372\n",
      "Epoch 82, change: 0.00837958\n",
      "Epoch 83, change: 0.00831172\n",
      "Epoch 84, change: 0.00821401\n",
      "Epoch 85, change: 0.00812508\n",
      "Epoch 86, change: 0.00799782\n",
      "Epoch 87, change: 0.00796785\n",
      "Epoch 88, change: 0.00785040\n",
      "Epoch 89, change: 0.00785452\n",
      "Epoch 90, change: 0.00769005\n",
      "Epoch 91, change: 0.00768520\n",
      "Epoch 92, change: 0.00756639\n",
      "Epoch 93, change: 0.00754052\n",
      "Epoch 94, change: 0.00743260\n",
      "Epoch 95, change: 0.00739293\n",
      "Epoch 96, change: 0.00730142\n",
      "Epoch 97, change: 0.00722954\n",
      "Epoch 98, change: 0.00717085\n",
      "Epoch 99, change: 0.00715821\n",
      "Epoch 100, change: 0.00703003\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.28263523\n",
      "Epoch 3, change: 0.20017374\n",
      "Epoch 4, change: 0.14112785\n",
      "Epoch 5, change: 0.10444240\n",
      "Epoch 6, change: 0.09170180\n",
      "Epoch 7, change: 0.07462611\n",
      "Epoch 8, change: 0.06476905\n",
      "Epoch 9, change: 0.05928636\n",
      "Epoch 10, change: 0.05305656\n",
      "Epoch 11, change: 0.04941997\n",
      "Epoch 12, change: 0.04483923\n",
      "Epoch 13, change: 0.04218562\n",
      "Epoch 14, change: 0.03888811\n",
      "Epoch 15, change: 0.03596616\n",
      "Epoch 16, change: 0.03444401\n",
      "Epoch 17, change: 0.03225128\n",
      "Epoch 18, change: 0.03026599\n",
      "Epoch 19, change: 0.02875379\n",
      "Epoch 20, change: 0.02720552\n",
      "Epoch 21, change: 0.02618582\n",
      "Epoch 22, change: 0.02492013\n",
      "Epoch 23, change: 0.02388698\n",
      "Epoch 24, change: 0.02303672\n",
      "Epoch 25, change: 0.02205500\n",
      "Epoch 26, change: 0.02107081\n",
      "Epoch 27, change: 0.02029344\n",
      "Epoch 28, change: 0.01939828\n",
      "Epoch 29, change: 0.01896609\n",
      "Epoch 30, change: 0.01822447\n",
      "Epoch 31, change: 0.01768305\n",
      "Epoch 32, change: 0.01719601\n",
      "Epoch 33, change: 0.01653859\n",
      "Epoch 34, change: 0.01607918\n",
      "Epoch 35, change: 0.01547343\n",
      "Epoch 36, change: 0.01510924\n",
      "Epoch 37, change: 0.01472718\n",
      "Epoch 38, change: 0.01430466\n",
      "Epoch 39, change: 0.01389965\n",
      "Epoch 40, change: 0.01365710\n",
      "Epoch 41, change: 0.01318217\n",
      "Epoch 42, change: 0.01291108\n",
      "Epoch 43, change: 0.01252743\n",
      "Epoch 44, change: 0.01231320\n",
      "Epoch 45, change: 0.01200071\n",
      "Epoch 46, change: 0.01174963\n",
      "Epoch 47, change: 0.01156728\n",
      "Epoch 48, change: 0.01132467\n",
      "Epoch 49, change: 0.01116841\n",
      "Epoch 50, change: 0.01095225\n",
      "Epoch 51, change: 0.01076731\n",
      "Epoch 52, change: 0.01066764\n",
      "Epoch 53, change: 0.01048327\n",
      "Epoch 54, change: 0.01024566\n",
      "Epoch 55, change: 0.01009726\n",
      "Epoch 56, change: 0.00995971\n",
      "Epoch 57, change: 0.00987306\n",
      "Epoch 58, change: 0.00964643\n",
      "Epoch 59, change: 0.00954892\n",
      "Epoch 60, change: 0.00942185\n",
      "Epoch 61, change: 0.00926338\n",
      "Epoch 62, change: 0.00912711\n",
      "Epoch 63, change: 0.00900817\n",
      "Epoch 64, change: 0.00883549\n",
      "Epoch 65, change: 0.00884248\n",
      "Epoch 66, change: 0.00861769\n",
      "Epoch 67, change: 0.00855625\n",
      "Epoch 68, change: 0.00843774\n",
      "Epoch 69, change: 0.00832172\n",
      "Epoch 70, change: 0.00821168\n",
      "Epoch 71, change: 0.00809832\n",
      "Epoch 72, change: 0.00801282\n",
      "Epoch 73, change: 0.00789258\n",
      "Epoch 74, change: 0.00781231\n",
      "Epoch 75, change: 0.00772475\n",
      "Epoch 76, change: 0.00757324\n",
      "Epoch 77, change: 0.00751068\n",
      "Epoch 78, change: 0.00743498\n",
      "Epoch 79, change: 0.00732136\n",
      "Epoch 80, change: 0.00725575\n",
      "Epoch 81, change: 0.00721029\n",
      "Epoch 82, change: 0.00708707\n",
      "Epoch 83, change: 0.00703386\n",
      "Epoch 84, change: 0.00694936\n",
      "Epoch 85, change: 0.00685038\n",
      "Epoch 86, change: 0.00681104\n",
      "Epoch 87, change: 0.00671472\n",
      "Epoch 88, change: 0.00666066\n",
      "Epoch 89, change: 0.00658379\n",
      "Epoch 90, change: 0.00651522\n",
      "Epoch 91, change: 0.00643953\n",
      "Epoch 92, change: 0.00637865\n",
      "Epoch 93, change: 0.00629528\n",
      "Epoch 94, change: 0.00622497\n",
      "Epoch 95, change: 0.00618723\n",
      "Epoch 96, change: 0.00610127\n",
      "Epoch 97, change: 0.00606360\n",
      "Epoch 98, change: 0.00603023\n",
      "Epoch 99, change: 0.00595351\n",
      "Epoch 100, change: 0.00586055\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.29885421\n",
      "Epoch 3, change: 0.18104433\n",
      "Epoch 4, change: 0.12453684\n",
      "Epoch 5, change: 0.10521583\n",
      "Epoch 6, change: 0.08338297\n",
      "Epoch 7, change: 0.07477170\n",
      "Epoch 8, change: 0.06465083\n",
      "Epoch 9, change: 0.05754526\n",
      "Epoch 10, change: 0.05095667\n",
      "Epoch 11, change: 0.04598933\n",
      "Epoch 12, change: 0.04215749\n",
      "Epoch 13, change: 0.03833610\n",
      "Epoch 14, change: 0.03525922\n",
      "Epoch 15, change: 0.03320409\n",
      "Epoch 16, change: 0.03065241\n",
      "Epoch 17, change: 0.02827714\n",
      "Epoch 18, change: 0.02674294\n",
      "Epoch 19, change: 0.02544714\n",
      "Epoch 20, change: 0.02412743\n",
      "Epoch 21, change: 0.02330770\n",
      "Epoch 22, change: 0.02239591\n",
      "Epoch 23, change: 0.02164310\n",
      "Epoch 24, change: 0.02106131\n",
      "Epoch 25, change: 0.02041937\n",
      "Epoch 26, change: 0.01998039\n",
      "Epoch 27, change: 0.01960886\n",
      "Epoch 28, change: 0.01907357\n",
      "Epoch 29, change: 0.01850158\n",
      "Epoch 30, change: 0.01831822\n",
      "Epoch 31, change: 0.01780102\n",
      "Epoch 32, change: 0.01738349\n",
      "Epoch 33, change: 0.01715819\n",
      "Epoch 34, change: 0.01663046\n",
      "Epoch 35, change: 0.01656225\n",
      "Epoch 36, change: 0.01611701\n",
      "Epoch 37, change: 0.01571251\n",
      "Epoch 38, change: 0.01549819\n",
      "Epoch 39, change: 0.01514416\n",
      "Epoch 40, change: 0.01490863\n",
      "Epoch 41, change: 0.01468903\n",
      "Epoch 42, change: 0.01435291\n",
      "Epoch 43, change: 0.01413821\n",
      "Epoch 44, change: 0.01385522\n",
      "Epoch 45, change: 0.01355532\n",
      "Epoch 46, change: 0.01332929\n",
      "Epoch 47, change: 0.01330493\n",
      "Epoch 48, change: 0.01288262\n",
      "Epoch 49, change: 0.01280861\n",
      "Epoch 50, change: 0.01255744\n",
      "Epoch 51, change: 0.01237618\n",
      "Epoch 52, change: 0.01221999\n",
      "Epoch 53, change: 0.01201493\n",
      "Epoch 54, change: 0.01177969\n",
      "Epoch 55, change: 0.01160978\n",
      "Epoch 56, change: 0.01150733\n",
      "Epoch 57, change: 0.01136693\n",
      "Epoch 58, change: 0.01112257\n",
      "Epoch 59, change: 0.01100757\n",
      "Epoch 60, change: 0.01073942\n",
      "Epoch 61, change: 0.01055805\n",
      "Epoch 62, change: 0.01046903\n",
      "Epoch 63, change: 0.01021254\n",
      "Epoch 64, change: 0.01006222\n",
      "Epoch 65, change: 0.00995806\n",
      "Epoch 66, change: 0.00967758\n",
      "Epoch 67, change: 0.00951671\n",
      "Epoch 68, change: 0.00947492\n",
      "Epoch 69, change: 0.00931065\n",
      "Epoch 70, change: 0.00908910\n",
      "Epoch 71, change: 0.00902031\n",
      "Epoch 72, change: 0.00884235\n",
      "Epoch 73, change: 0.00875444\n",
      "Epoch 74, change: 0.00860555\n",
      "Epoch 75, change: 0.00847289\n",
      "Epoch 76, change: 0.00838869\n",
      "Epoch 77, change: 0.00833116\n",
      "Epoch 78, change: 0.00819022\n",
      "Epoch 79, change: 0.00812822\n",
      "Epoch 80, change: 0.00808339\n",
      "Epoch 81, change: 0.00793725\n",
      "Epoch 82, change: 0.00787113\n",
      "Epoch 83, change: 0.00777321\n",
      "Epoch 84, change: 0.00767763\n",
      "Epoch 85, change: 0.00760841\n",
      "Epoch 86, change: 0.00746663\n",
      "Epoch 87, change: 0.00740593\n",
      "Epoch 88, change: 0.00738852\n",
      "Epoch 89, change: 0.00726646\n",
      "Epoch 90, change: 0.00722305\n",
      "Epoch 91, change: 0.00709121\n",
      "Epoch 92, change: 0.00705962\n",
      "Epoch 93, change: 0.00694287\n",
      "Epoch 94, change: 0.00691872\n",
      "Epoch 95, change: 0.00680598\n",
      "Epoch 96, change: 0.00674342\n",
      "Epoch 97, change: 0.00668984\n",
      "Epoch 98, change: 0.00659945\n",
      "Epoch 99, change: 0.00652496\n",
      "Epoch 100, change: 0.00648123\n",
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.26657975\n",
      "Epoch 3, change: 0.17615789\n",
      "Epoch 4, change: 0.13130680\n",
      "Epoch 5, change: 0.10881248\n",
      "Epoch 6, change: 0.08714379\n",
      "Epoch 7, change: 0.08214275\n",
      "Epoch 8, change: 0.07072963\n",
      "Epoch 9, change: 0.06401235\n",
      "Epoch 10, change: 0.05741809\n",
      "Epoch 11, change: 0.05076340\n",
      "Epoch 12, change: 0.04653978\n",
      "Epoch 13, change: 0.04225387\n",
      "Epoch 14, change: 0.03862270\n",
      "Epoch 15, change: 0.03492701\n",
      "Epoch 16, change: 0.03298784\n",
      "Epoch 17, change: 0.03080567\n",
      "Epoch 18, change: 0.02927496\n",
      "Epoch 19, change: 0.02784887\n",
      "Epoch 20, change: 0.02659162\n",
      "Epoch 21, change: 0.02540661\n",
      "Epoch 22, change: 0.02440795\n",
      "Epoch 23, change: 0.02355759\n",
      "Epoch 24, change: 0.02261444\n",
      "Epoch 25, change: 0.02159863\n",
      "Epoch 26, change: 0.02080975\n",
      "Epoch 27, change: 0.02018592\n",
      "Epoch 28, change: 0.01953526\n",
      "Epoch 29, change: 0.01878998\n",
      "Epoch 30, change: 0.01835240\n",
      "Epoch 31, change: 0.01774167\n",
      "Epoch 32, change: 0.01722140\n",
      "Epoch 33, change: 0.01685276\n",
      "Epoch 34, change: 0.01653553\n",
      "Epoch 35, change: 0.01614502\n",
      "Epoch 36, change: 0.01575774\n",
      "Epoch 37, change: 0.01540908\n",
      "Epoch 38, change: 0.01498954\n",
      "Epoch 39, change: 0.01474574\n",
      "Epoch 40, change: 0.01444226\n",
      "Epoch 41, change: 0.01409534\n",
      "Epoch 42, change: 0.01388458\n",
      "Epoch 43, change: 0.01368262\n",
      "Epoch 44, change: 0.01335455\n",
      "Epoch 45, change: 0.01311790\n",
      "Epoch 46, change: 0.01284474\n",
      "Epoch 47, change: 0.01264807\n",
      "Epoch 48, change: 0.01239522\n",
      "Epoch 49, change: 0.01220936\n",
      "Epoch 50, change: 0.01192506\n",
      "Epoch 51, change: 0.01183912\n",
      "Epoch 52, change: 0.01162102\n",
      "Epoch 53, change: 0.01144515\n",
      "Epoch 54, change: 0.01125402\n",
      "Epoch 55, change: 0.01107900\n",
      "Epoch 56, change: 0.01091599\n",
      "Epoch 57, change: 0.01071080\n",
      "Epoch 58, change: 0.01058419\n",
      "Epoch 59, change: 0.01036218\n",
      "Epoch 60, change: 0.01022682\n",
      "Epoch 61, change: 0.01011888\n",
      "Epoch 62, change: 0.01006122\n",
      "Epoch 63, change: 0.00993280\n",
      "Epoch 64, change: 0.00980848\n",
      "Epoch 65, change: 0.00978767\n",
      "Epoch 66, change: 0.00963262\n",
      "Epoch 67, change: 0max_iter reached after 282 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 276 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 283 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 277 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 274 seconds\n",
      "\n",
      " with l1 ration 1: \n",
      "0.3711333333333333\n",
      "LogisticRegression(l1_ratio=0.85, multi_class='multinomial',\n",
      "                   penalty='elasticnet', solver='saga', verbose=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 307 seconds\n",
      "\n",
      " HELLO\n",
      "0.3824\n",
      "LogisticRegression(l1_ratio=0.85, multi_class='multinomial',\n",
      "                   penalty='elasticnet', solver='saga', verbose=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.1min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "print(\"Fetching data + {}\".format(time.time()))\n",
    "# Fetch the data\n",
    "cifar = fetch_openml('cifar_10_small')\n",
    "cifar['categories'] = {\n",
    "    '0' : 'airplane',\n",
    "    '1' : 'automobile',\n",
    "    '2' : 'bird',\n",
    "    '3' : 'cat',\n",
    "    '4' : 'deer',\n",
    "    '5' : 'dog',\n",
    "    '6' : 'frog',\n",
    "    '7' : 'horse',\n",
    "    '8' : 'ship',\n",
    "    '9' : 'truck',\n",
    "}\n",
    "\n",
    "print(\"Splitting data + {}\".format(time.time()))\n",
    "# Test train split\n",
    "X_train, X_test = train_test_split(cifar['data'], test_size=0.25, random_state=0)\n",
    "Y_train, Y_test = train_test_split(cifar['target'], test_size=0.25, random_state=0)\n",
    "\n",
    "train_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# distribution = len(train_labels)\n",
    "# for category, size in zip(distribution.index, distribution.values):\n",
    "#     print(f\"{category} {size} images\")\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# train_labels[\"label\"].value_counts().plot(kind='bar', title='Distribution of classes')\n",
    "\n",
    "## image display - work by Jackson\n",
    "def display_image_grid(dataset,\n",
    "                        grid_width  = 5,\n",
    "                        grid_height = 5,\n",
    "                        img_width   = 32,\n",
    "                        img_height  = 32,\n",
    "                        figsize = 2.0,\n",
    "                        _format = 'RGB'):\n",
    "    fig, ax = plt.subplots(grid_height, grid_width, figsize=(figsize*grid_width, figsize*grid_height), facecolor='gray')\n",
    "\n",
    "    for m in range(grid_height):\n",
    "        for n in range(grid_width):\n",
    "            i = np.random.choice(len(dataset['data']))\n",
    "            ax[m][n].set_axis_off()\n",
    "\n",
    "            if type(dataset['categories']) == dict:\n",
    "                ax[m][n].set_title('%s: %s'%(i,dataset['categories'][dataset['target'].iloc[i]]))\n",
    "            else:\n",
    "                ax[m][n].set_title('%s: %s'%(i, dataset['target'].iloc[i]))\n",
    "\n",
    "            im = np.array(dataset['data'].iloc[i]).astype('uint8')\n",
    "            if _format == 'RGB':\n",
    "                im = im.reshape((img_width, img_height, 3), order='F')\n",
    "                im = np.swapaxes(im, 0, 1)\n",
    "                ax[m][n].imshow(im)\n",
    "\n",
    "            elif _format == 'grayscale':\n",
    "                im = im.reshape((img_width, img_height), order='F')\n",
    "                im = np.swapaxes(im, 0, 1)\n",
    "                ax[m][n].imshow(im, cmap='gray')\n",
    "            else:\n",
    "                raise Exception('_format MUST be either RGB or grayscale')\n",
    "\n",
    "train = {\n",
    "    'data': X_train,\n",
    "    'target': Y_train,\n",
    "    'categories': cifar['categories']\n",
    "}\n",
    "# display_image_grid(train, grid_width = 5, grid_height = 3, figsize=3)\n",
    "\n",
    "print(\"Skipped image creation + {}\".format(time.time()))\n",
    "\n",
    "print(\"starting log reg + {}\".format(time.time()))\n",
    "\n",
    "# # Logistic Regression\n",
    "log_reg_model = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=0.5)\n",
    "#log_reg_model.fit(X_train, Y_train)\n",
    "#predictions = log_reg_model.predict(X_test)\n",
    "# scores = model_selection.cross_val_score(log_reg_model, X_train, Y_train, cv=10)\n",
    "# print(scores)\n",
    "# print('average score: {}'.format(scores.mean()))\n",
    "\n",
    "preds = model_selection.cross_val_predict(log_reg_model, X_train, Y_train, cv=10)\n",
    "print('\\n with l1 ratio 0.5: best training accuracy ')\n",
    "print(metrics.accuracy_score(Y_train, preds))\n",
    "\n",
    "log_reg_model_2 = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=1)\n",
    "preds2 = model_selection.cross_val_predict(log_reg_model_2, X_train, Y_train, cv=10)\n",
    "print('\\n with l1-ratio 1: best training accuracy')\n",
    "print(metrics.accuracy_score(Y_train, preds2))\n",
    "\n",
    "log_reg_model_3 = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=0)\n",
    "preds3 = model_selection.cross_val_predict(log_reg_model_3, X_train, Y_train, cv=10)\n",
    "print('\\n with l1-ration 0: best training accuracy')\n",
    "print(metrics.accuracy_score(Y_train, preds3))\n",
    "\n",
    "log_reg_model_4 = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=0.85)\n",
    "preds2 = model_selection.cross_val_predict(log_reg_model_4, X_train, Y_train, cv=10)\n",
    "print('\\n with l1 ration 0.85: best training accuracy')\n",
    "print(metrics.accuracy_score(Y_train, preds2))\n",
    "print(log_reg_model_4)\n",
    "\n",
    "log_reg_model_4.fit(X_train, Y_train)\n",
    "#Testing Accuracy:\n",
    "print(\"\\n Using the model with the best training accuracy: the corresponding test error: \")\n",
    "predictions = log_reg_model_4.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test, predictions))\n",
    "print(log_reg_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With about a li-ratio of 0.5: 0.37106666666666666 was the best accuracy.\n",
      "\n",
      "With a li-ratio of 0:  0.37126666666666667 was the best accuracy.\n",
      "\n",
      "With a li-ratio of 1:  0.37153333333333333 was the best accuracy. \n",
      "\n",
      "With a  l1-ratio of 0.85:  0.3716 was the best accuracy. \n",
      "\n",
      "As such, setting the elastic-net mixing parameter to a l1 ratio penalty of 0.85 gave the best training accuracy.With that model, the best test accuracy is 0.3824.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWith about a li-ratio of 0.5: 0.37106666666666666 was the best accuracy.\")\n",
    "print(\"\\nWith a li-ratio of 0:  0.37126666666666667 was the best accuracy.\")\n",
    "print(\"\\nWith a li-ratio of 1:  0.37153333333333333 was the best accuracy. \")\n",
    "print(\"\\nWith a  l1-ratio of 0.85:  0.3716 was the best accuracy. \")\n",
    "print(\"\\nAs such, setting the elastic-net mixing parameter to a l1 ratio penalty of 0.85 gave the best training accuracy.With that model, the best test accuracy is 0.3824.\")\n",
    "print(\"\\nThe elasticnet model trains with both l1 and l2 norms (mixes both Lasso and Ridge Regularization allowing for a sparser model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 2: Multi-class Logistic Regression – Visualizing the Solution.  - Josh\n",
    "You will repeat\n",
    "the previous problem but for the MNIST dataset which you will find here: https://www.openml.\n",
    "org/d/554. MNIST is a dataset of handwritten digits, and is considered one of the easiest image\n",
    "recognition problems in computer vision. We will see here how well logistic regression does, as you\n",
    "did above on the CIFAR-10 subset. In addition, we will see that we can visualize the solution, and\n",
    "that in connection to this, sparsity can be useful.\n",
    "•Use the fetch openml command from sklearn.datasets to import the MNIST data set,\n",
    "•Choose a reasonable train-test split, and again run multi-class logistic regression on these\n",
    "using the cross entropy loss, as you did above. Try to optimize the hyperparameters.\n",
    "•Report your training and test loss from above,\n",
    "•Choose an ℓ1 regularizer (penalty), and see if you can get a sparse solution with almost as\n",
    "good accuracy.\n",
    "•Note that in Logistic Regression, the coefficients returned (i.e., the β’s) are the same dimen-\n",
    "sion as the data. Therefore we can pretend that the coefficients of the solution are an image\n",
    "of the same dimension, and plot it. Do this for the 10 sets of coefficients that correspond to\n",
    "the 10 classes. You should observe that, at least for the sparse solutions, these “kind of” look\n",
    "like the digits they are classifying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 3: Revisiting Logistic Regression and MNIST. - Josh\n",
    "Here we throw the kitchen sink of classical ML (i.e. pre-deep learning) on MNIST.\n",
    "•Use Random Forests to try to get the best possible test accuracy on MNIST. Use Cross\n",
    "Validation to find the best settings. How well can you do? You should use the accuracy\n",
    "metric to compare to logistic regression. What are the hyperparameters of your best model?\n",
    "•Use Gradient Boosting to do the same. Try your best to tune your hyper parameters. What\n",
    "are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 4: Revisiting Logistic Regression and CIFAR-10. - Jackson\n",
    "As before, we’ll throw the kitchen sink of classical ML (i.e. pre-deep learning) on CIFAR-10.  \n",
    "Keep in mind that CIFAR-10 is a few times larger.\n",
    "* What is the best accuracy you can get on the test data, by tuning Random Forests? \n",
    "    * What are the hyperparameters of your best model?\n",
    "* What is the best accuracy you can get on the test data, by tuning any model including Gradient boosting? \n",
    "    * What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 5: Getting Started with Pytorch. - Jackson\n",
    " * Install Pytorch.\n",
    " * Work through this tutorial to familiarize yourself with Pytorch basics: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    " * Work through this tutorial on MNIST starting from a Pytorch logistic regression and building to a CNN using torch.nn. Use a GPU (e.g. on Colab, through Google Cloud credits, Pa-perspace, or any other way). https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    " * Design the best CNN you can to get the best accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 6: CNNs for CIFAR-10. - Jackson\n",
    "* Build a CNN and optimize the accuracy for CIFAR-10. \n",
    "    * Try different number of layers and different architectures (depth and convolutional filter hyperparameters).\n",
    "* Is momentum and learning rate having a significant effect? \n",
    "    * Track the train and test loss across training epochs and plot them for different learning rates and momentum values.\n",
    "* Is the depth of the CNN having a significant effect on performance? \n",
    "    * Describe the hyperparameters of the best model you could train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}