{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 0 - Jhanvi\n",
    "\n",
    "As we have covered in class, we are training a logistic regression model to predict if\n",
    "someone will click on an advertisement. Consider the logistic regression model with 3 features and\n",
    "weights w = [1, −30, 3].\n",
    "\n",
    "For the dataset with features\n",
    "x1=[20,0,0], y1=1\n",
    "x2=[23,1,1], y2=0,\n",
    "•Compute the probabilities that the logistic regression assigns to these two customers clicking\n",
    "on the advertisement (i.e. y=1)\n",
    "•Compute the cross entropy loss of this logistic regression.\n",
    "•Design a decision stump (a decision tree of depth 1) that splits on the first feature. What is\n",
    "the Gini impurity of the root? What is the Gini impurity after the best split that you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logisitic regression assigned the following probabilities to the customers: \n",
      "[1.         0.01798621]\n",
      "The cross entropy loss is: \n",
      "2.0611536942919273e-09\n",
      "\n",
      "To reflect, the model predicts that the first customer will click on an advertisement with very high probability and is correct. As such, the loss is very small.\n",
      "x[0][0]: 20\n",
      "x[1][0]: 23\n",
      "The gini impurity for the left and right buckets at a split of 21.5 is (0.0, 0.0)\n",
      "\n",
      "The impurity calculated is 0, which is the best possible Gini impurity for a bucket, which indicates that all the elements in that bucket are of the same class.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "w = ([1, -30, 3])\n",
    "x = ([20, 0, 0], [23, 1, 1])\n",
    "y = ([1, 0])\n",
    "\n",
    "def logistic_reg(w, x):\n",
    "    col_x = np.transpose(x)\n",
    "    scores = np.dot(w, col_x)\n",
    "\n",
    "    sigmoid_1 = float(1 / (1 + math.exp(-1 * scores[0])))\n",
    "    sigmoid_2 = float(1 / (1 + math.exp(-1 * scores[1])))\n",
    "\n",
    "    sigmoid_matrix = np.array([sigmoid_1, sigmoid_2])\n",
    "\n",
    "    return sigmoid_matrix\n",
    "\n",
    "def cross_entropy(y, sigmoid_matrix):\n",
    "    probability = (y[0] * sigmoid_matrix[0]) + (y[1] * sigmoid_matrix[1])\n",
    "    return -1 * math.log(probability)\n",
    "\n",
    "sigmoid_matrix = logistic_reg(w, x)\n",
    "loss = cross_entropy(np.transpose(y), sigmoid_matrix)\n",
    "\n",
    "print(\"The logisitic regression assigned the following probabilities to the customers: \")\n",
    "print(sigmoid_matrix)\n",
    "print(\"The cross entropy loss is: \")\n",
    "print(loss)\n",
    "\n",
    "print(\"\\nTo reflect, the model predicts that the first customer will click on an advertisement with very high probability and is correct. As such, the loss is very small.\")\n",
    "\n",
    "def decision_tree(x):\n",
    "    col_x = np.transpose(x)\n",
    "    split = (x[0][0] + x[1][0]) / 2\n",
    "    print(\"x[0][0]: \" + str(x[0][0]))\n",
    "    print(\"x[1][0]: \" + str(x[1][0]))\n",
    "\n",
    "    if x[0][0] > split:\n",
    "        right = 0\n",
    "    else :\n",
    "        left = 0\n",
    "\n",
    "    if x[1][0] > split:\n",
    "        right = 1\n",
    "    else :\n",
    "        left = 1\n",
    "\n",
    "    return left, right, split\n",
    "\n",
    "def gini_impurity(l_bucket, r_bucket):\n",
    "    num_0_left, num_1_right, num_0_right, num_1_left = 0, 0, 0, 0\n",
    "\n",
    "    if l_bucket == 0:\n",
    "        num_0_left += 1\n",
    "    if r_bucket == 1:\n",
    "        num_1_right += 1\n",
    "    bucket_total = 1\n",
    "\n",
    "    gini_left = ((num_0_left/bucket_total) * (1 - num_0_left/bucket_total)) + ((num_1_left/bucket_total) * (1 - num_1_left/bucket_total))\n",
    "    gini_right = ((num_0_right/bucket_total) * (1 - num_0_right/bucket_total)) + ((num_1_right/bucket_total) * (1 - num_1_right/bucket_total))\n",
    "\n",
    "    return gini_left, gini_right\n",
    "\n",
    "left, right, split = decision_tree(x)\n",
    "gini = gini_impurity(left, right)\n",
    "print(\"The gini impurity for the left and right buckets at a split of {} is {}\".format(split, gini))\n",
    "print(\"\\nThe impurity calculated is 0, which is the best possible Gini impurity for a bucket, which indicates that all the elements in that bucket are of the same class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 1: Logistic Regression and CIFAR-10. - Jhanvi\n",
    "In this problem you will explore the dataset\n",
    "CIFAR-10, and you will use multinomial (multi-label) Logistic Regression to try to classify it. You\n",
    "will also explore visualizing the solution.\n",
    "\n",
    "(Optional) You can read about the CIFAR-10 and CIFAR-100 datasets here: https://www.\n",
    "cs.toronto.edu/~kriz/cifar.html.\n",
    "•(Optional) OpenML curates a number of data sets. You will use a subset of CIFAR-10\n",
    "provided by them. Read here for a description: https://www.openml.org/d/40926.\n",
    "•Use the fetch openml command from sklearn.datasets to import the CIFAR-10-Small\n",
    "data set.\n",
    "•Figure out how to display some of the images in this data set, and display a couple. While\n",
    "not high resolution, these should be recognizable if you are doing it correctly.\n",
    "•There are 20,000 data points. Do a train-test split on 3/4 - 1/4.\n",
    "•You will run multi-class logistic regression on these using the cross entropy loss. You have to\n",
    "specify this specifically (multi class=’multinomial’). Use cross validation to see how good\n",
    "your accuracy can be. In this case, cross validate to find as good regularization coefficients\n",
    "as you can, for ℓ1 and ℓ2 regularization (called penalties), which are naturally supported in\n",
    "sklearn.linear model.LogisticRegression. I recommend you use the solver saga.\n",
    "•Report your training and test loss from above,\n",
    "•How sparse can you make your solutions without deteriorating your testing error too much?\n",
    "Here, we ask for a sparse solution that has test accuracy that is close to the best solution you\n",
    "found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data + 1665363398.507539\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching data + \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fetch the data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cifar \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_openml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar_10_small\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m cifar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairplane\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautomobile\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruck\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m }\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplitting data + \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()))\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/datasets/_openml.py:842\u001b[0m, in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# obtain the data\u001b[39;00m\n\u001b[1;32m    841\u001b[0m url \u001b[38;5;241m=\u001b[39m _DATA_FILE\u001b[38;5;241m.\u001b[39mformat(data_description[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 842\u001b[0m bunch \u001b[38;5;241m=\u001b[39m \u001b[43m_download_data_to_bunch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mas_frame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_description\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5_checksum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_X_y:\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bunch\u001b[38;5;241m.\u001b[39mdata, bunch\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/datasets/_openml.py:520\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, features_list, data_columns, target_columns, shape, md5_checksum, n_retries, delay)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     output_arrays_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 520\u001b[0m X, y, frame, nominal_attributes \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_with_clean_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_load_arff_response\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_arrays_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_slice_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_slice_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\n\u001b[1;32m    538\u001b[0m     data\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    539\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     target_names\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[1;32m    544\u001b[0m )\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/datasets/_openml.py:52\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/datasets/_openml.py:446\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, output_arrays_type, features_dict, data_columns, target_columns, col_slice_x, col_slice_y, shape, md5_checksum, n_retries, delay)\u001b[0m\n\u001b[1;32m    440\u001b[0m return_type \u001b[38;5;241m=\u001b[39m _arff\u001b[38;5;241m.\u001b[39mCOO \u001b[38;5;28;01mif\u001b[39;00m output_arrays_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m _arff\u001b[38;5;241m.\u001b[39mDENSE_GEN\n\u001b[1;32m    442\u001b[0m arff \u001b[38;5;241m=\u001b[39m _arff\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    443\u001b[0m     stream, return_type\u001b[38;5;241m=\u001b[39mreturn_type, encode_nominal\u001b[38;5;241m=\u001b[39mencode_nominal\n\u001b[1;32m    444\u001b[0m )\n\u001b[0;32m--> 446\u001b[0m X, y, frame, nominal_attributes \u001b[38;5;241m=\u001b[39m \u001b[43m_liac_arff_parser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43marff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_arrays_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_slice_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_slice_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# consume remaining stream, if early exited\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m stream:\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/datasets/_arff_parser.py:223\u001b[0m, in \u001b[0;36m_liac_arff_parser\u001b[0;34m(arff_container, output_arrays_type, features_dict, data_columns, target_columns, col_slice_x, col_slice_y, shape)\u001b[0m\n\u001b[1;32m    221\u001b[0m nominal_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    222\u001b[0m columns \u001b[38;5;241m=\u001b[39m data_columns \u001b[38;5;241m+\u001b[39m target_columns\n\u001b[0;32m--> 223\u001b[0m (frame,) \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_arff_data_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marff_container\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m X \u001b[38;5;241m=\u001b[39m frame[data_columns]\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target_columns) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/datasets/_arff_parser.py:193\u001b[0m, in \u001b[0;36m_convert_arff_data_dataframe\u001b[0;34m(arff, columns, features_dict)\u001b[0m\n\u001b[1;32m    191\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    192\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(first_df[columns_to_keep])\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m _chunk_generator(arff[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunksize):\n\u001b[1;32m    194\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39marff_columns)[columns_to_keep])\n\u001b[1;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/utils/__init__.py:688\u001b[0m, in \u001b[0;36m_chunk_generator\u001b[0;34m(gen, chunksize)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03mchunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 688\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/externals/_arff.py:474\u001b[0m, in \u001b[0;36mDenseGeneratorData.decode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(conversors):\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BadDataFormat(row)\n\u001b[0;32m--> 474\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/externals/_arff.py:479\u001b[0m, in \u001b[0;36mDenseGeneratorData._decode_values\u001b[0;34m(values, conversors)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_values\u001b[39m(values, conversors):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m conversor(value)\n\u001b[1;32m    480\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m conversor, value\n\u001b[1;32m    481\u001b[0m                   \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(conversors, values)]\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(exc):\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/sklearn/externals/_arff.py:479\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_values\u001b[39m(values, conversors):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mconversor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m conversor, value\n\u001b[1;32m    481\u001b[0m                   \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(conversors, values)]\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(exc):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "print(\"Fetching data + {}\".format(time.time()))\n",
    "# Fetch the data\n",
    "cifar = fetch_openml('cifar_10_small')\n",
    "cifar['categories'] = {\n",
    "    '0' : 'airplane',\n",
    "    '1' : 'automobile',\n",
    "    '2' : 'bird',\n",
    "    '3' : 'cat',\n",
    "    '4' : 'deer',\n",
    "    '5' : 'dog',\n",
    "    '6' : 'frog',\n",
    "    '7' : 'horse',\n",
    "    '8' : 'ship',\n",
    "    '9' : 'truck',\n",
    "}\n",
    "\n",
    "print(\"Splitting data + {}\".format(time.time()))\n",
    "# Test train split\n",
    "X_train, X_test = train_test_split(cifar['data'], test_size=0.25, random_state=0)\n",
    "Y_train, Y_test = train_test_split(cifar['target'], test_size=0.25, random_state=0)\n",
    "\n",
    "train_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# distribution = len(train_labels)\n",
    "# for category, size in zip(distribution.index, distribution.values):\n",
    "#     print(f\"{category} {size} images\")\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# train_labels[\"label\"].value_counts().plot(kind='bar', title='Distribution of classes')\n",
    "\n",
    "## image display - work by Jackson\n",
    "def display_image_grid(dataset,\n",
    "                        grid_width  = 5,\n",
    "                        grid_height = 5,\n",
    "                        img_width   = 32,\n",
    "                        img_height  = 32,\n",
    "                        figsize = 2.0,\n",
    "                        _format = 'RGB'):\n",
    "    fig, ax = plt.subplots(grid_height, grid_width, figsize=(figsize*grid_width, figsize*grid_height), facecolor='gray')\n",
    "\n",
    "    for m in range(grid_height):\n",
    "        for n in range(grid_width):\n",
    "            i = np.random.choice(len(dataset['data']))\n",
    "            ax[m][n].set_axis_off()\n",
    "\n",
    "            if type(dataset['categories']) == dict:\n",
    "                ax[m][n].set_title('%s: %s'%(i,dataset['categories'][dataset['target'].iloc[i]]))\n",
    "            else:\n",
    "                ax[m][n].set_title('%s: %s'%(i, dataset['target'].iloc[i]))\n",
    "\n",
    "            im = np.array(dataset['data'].iloc[i]).astype('uint8')\n",
    "            if _format == 'RGB':\n",
    "                im = im.reshape((img_width, img_height, 3), order='F')\n",
    "                im = np.swapaxes(im, 0, 1)\n",
    "                ax[m][n].imshow(im)\n",
    "\n",
    "            elif _format == 'grayscale':\n",
    "                im = im.reshape((img_width, img_height), order='F')\n",
    "                im = np.swapaxes(im, 0, 1)\n",
    "                ax[m][n].imshow(im, cmap='gray')\n",
    "            else:\n",
    "                raise Exception('_format MUST be either RGB or grayscale')\n",
    "\n",
    "train = {\n",
    "    'data': X_train,\n",
    "    'target': Y_train,\n",
    "    'categories': cifar['categories']\n",
    "}\n",
    "# display_image_grid(train, grid_width = 5, grid_height = 3, figsize=3)\n",
    "\n",
    "print(\"Skipped image creation + {}\".format(time.time()))\n",
    "\n",
    "print(\"starting log reg + {}\".format(time.time()))\n",
    "# Logistic Regression\n",
    "log_reg_model = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=0.5)\n",
    "#log_reg_model.fit(X_train, Y_train)\n",
    "#predictions = log_reg_model.predict(X_test)\n",
    "# scores = model_selection.cross_val_score(log_reg_model, X_train, Y_train, cv=10)\n",
    "# print(scores)\n",
    "# print('average score: {}'.format(scores.mean()))\n",
    "\n",
    "preds = model_selection.cross_val_predict(log_reg_model, X_train, Y_train, cv=10)\n",
    "print('\\n with l1 ration 0.5: ')\n",
    "print(metrics.accuracy_score(Y_train, preds))\n",
    "\n",
    "log_reg_model_2 = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=1)\n",
    "preds2 = model_selection.cross_val_predict(log_reg_model_2, X_train, Y_train, cv=10)\n",
    "print('\\n with l1 ration 1: ')\n",
    "print(metrics.accuracy_score(Y_train, preds2))\n",
    "\n",
    "log_reg_model_3 = LogisticRegression(penalty='elasticnet',solver='saga',multi_class='multinomial', verbose=1, l1_ratio=0)\n",
    "preds3 = model_selection.cross_val_predict(log_reg_model_3, X_train, Y_train, cv=10)\n",
    "print('\\n with l1 ration 0: ')\n",
    "print(metrics.accuracy_score(Y_train, preds3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With about a 1/2 1/2 L1 and L2 ratio 0.37106666666666666 was the best accuracy.\n",
      "\n",
      "With a full L2 ratio 0.37126666666666667 was the best accuracy.\n",
      "\n",
      " With a full L1 ratio 37153333333333333 was the best accuracy. \n",
      "As such, setting the elastic-net mixing parameter to a penalty of L1 gave the best training accuracy. \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWith about a 1/2 1/2 L1 and L2 ratio 0.37106666666666666 was the best accuracy.\")\n",
    "print(\"\\nWith a full L2 ratio 0.37126666666666667 was the best accuracy.\")\n",
    "print(\"\\n With a full L1 ratio 37153333333333333 was the best accuracy. \")\n",
    "print(\"As such, setting the elastic-net mixing parameter to a penalty of L1 gave the best training accuracy. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 2: Multi-class Logistic Regression – Visualizing the Solution.  - Josh\n",
    "You will repeat\n",
    "the previous problem but for the MNIST dataset which you will find here: https://www.openml.\n",
    "org/d/554. MNIST is a dataset of handwritten digits, and is considered one of the easiest image\n",
    "recognition problems in computer vision. We will see here how well logistic regression does, as you\n",
    "did above on the CIFAR-10 subset. In addition, we will see that we can visualize the solution, and\n",
    "that in connection to this, sparsity can be useful.\n",
    "•Use the fetch openml command from sklearn.datasets to import the MNIST data set,\n",
    "•Choose a reasonable train-test split, and again run multi-class logistic regression on these\n",
    "using the cross entropy loss, as you did above. Try to optimize the hyperparameters.\n",
    "•Report your training and test loss from above,\n",
    "•Choose an ℓ1 regularizer (penalty), and see if you can get a sparse solution with almost as\n",
    "good accuracy.\n",
    "•Note that in Logistic Regression, the coefficients returned (i.e., the β’s) are the same dimen-\n",
    "sion as the data. Therefore we can pretend that the coefficients of the solution are an image\n",
    "of the same dimension, and plot it. Do this for the 10 sets of coefficients that correspond to\n",
    "the 10 classes. You should observe that, at least for the sparse solutions, these “kind of” look\n",
    "like the digits they are classifying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 3: Revisiting Logistic Regression and MNIST. - Josh\n",
    "Here we throw the kitchen sink of classical ML (i.e. pre-deep learning) on MNIST.\n",
    "•Use Random Forests to try to get the best possible test accuracy on MNIST. Use Cross\n",
    "Validation to find the best settings. How well can you do? You should use the accuracy\n",
    "metric to compare to logistic regression. What are the hyperparameters of your best model?\n",
    "•Use Gradient Boosting to do the same. Try your best to tune your hyper parameters. What\n",
    "are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 4: Revisiting Logistic Regression and CIFAR-10. - Jackson\n",
    "As before, we’ll throw the kitchen sink of classical ML (i.e. pre-deep learning) on CIFAR-10.  \n",
    "Keep in mind that CIFAR-10 is a few times larger.\n",
    "* What is the best accuracy you can get on the test data, by tuning Random Forests? \n",
    "    * What are the hyperparameters of your best model?\n",
    "* What is the best accuracy you can get on the test data, by tuning any model including Gradient boosting? \n",
    "    * What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 5: Getting Started with Pytorch. - Jackson\n",
    " * Install Pytorch.\n",
    " * Work through this tutorial to familiarize yourself with Pytorch basics: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    " * Work through this tutorial on MNIST starting from a Pytorch logistic regression and building to a CNN using torch.nn. Use a GPU (e.g. on Colab, through Google Cloud credits, Pa-perspace, or any other way). https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    " * Design the best CNN you can to get the best accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 6: CNNs for CIFAR-10. - Jackson\n",
    "* Build a CNN and optimize the accuracy for CIFAR-10. \n",
    "    * Try different number of layers and different architectures (depth and convolutional filter hyperparameters).\n",
    "* Is momentum and learning rate having a significant effect? \n",
    "    * Track the train and test loss across training epochs and plot them for different learning rates and momentum values.\n",
    "* Is the depth of the CNN having a significant effect on performance? \n",
    "    * Describe the hyperparameters of the best model you could train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
