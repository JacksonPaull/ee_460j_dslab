{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Josh\n",
    "\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’.  \n",
    "Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication.\n",
    "https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "\n",
    "*Q: Summarize what you learned briefly (e.g. half a page).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<Summary\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Jackson  \n",
    "\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2017 papers from http://proceedings.mlr.press/v70/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "2. Let Zbe a randomly selected word in a randomly selected ICML paper. Estimate the entropy\n",
    "of Z.\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "4. (Extra credit) Synthesize a random paragraph using an n-gram model on words. Synthesize\n",
    "a random paragraph using any model you want. Top five synthesized text paragraphs win\n",
    "bonus (+30 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def scrape(source = 'http://proceedings.mlr.press/v70/', dump_folder = './scraped/'):\n",
    "    #Create folder to dump into\n",
    "    if not os.path.isdir(dump_folder):\n",
    "        os.mkdir(dump_folder)\n",
    "\n",
    "    #Set up logging\n",
    "    f = open(f'{dump_folder}log.txt', 'w') #Open the file if its not already opened\n",
    "    f.close()\n",
    "    logging.basicConfig(level=logging.INFO, filename=f'{dump_folder}log.txt')\n",
    "    \n",
    "    #Get list of links\n",
    "    html = requests.get(source)\n",
    "    soup = bs(html.content, 'html.parser')\n",
    "    links = soup.findAll('a')\n",
    "\n",
    "    #Scrape all PDFs\n",
    "    names = []\n",
    "    for l in links:\n",
    "        if l.decode_contents() == 'Download PDF' or l.decode_contents() == 'Supplementary PDF':\n",
    "            src = l.get('href').replace('ı', 'i')\n",
    "            fname = src[src.rindex('/')+1:]\n",
    "            if fname in names:\n",
    "                logging.CRITICAL(f'OVERWRITING FILE WITH NAME {fname}')\n",
    "            names.append(fname)\n",
    "            logging.info(f'Scraping pdf from {src} into {fname}')\n",
    "\n",
    "            pdf = requests.get(src)\n",
    "            with open(f'{dump_folder}{fname}', 'wb') as f:\n",
    "                f.write(pdf.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe \n",
    "    #This marks the word that prefaces the word in the data\n",
    "    #For every word mark a data entry (Including \\end)\n",
    "        #Word preface (Word or \\start)\n",
    "        #Track the count of times that a word comes after that word\n",
    "        #Use this to get marginal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "pdf = fitz.open('./scraped/achab17a-supp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplementary material of the article Uncovering Causality from\n",
      "Multivariate Hawkes Integrated Cumulants\n",
      "\n",
      "---\n",
      "\n",
      "Massil Achab∗1, Emmanuel Bacry1, St´ephane Gaiffas1, Iacopo Mastromatteo2, and Jean-Franc¸ois\n",
      "Muzy1,3\n",
      "\n",
      "---\n",
      "\n",
      "1Centre de Math´ematiques Appliqu´ees, CNRS, Ecole Polytechnique, UMR 7641, 91128 Palaiseau, France\n",
      "2Capital Fund Management, 23 rue de l’Universit´e, 75007 Paris, France\n",
      "3Laboratoire Sciences Pour l’Environnement, Universit´e de Corse, 7 Avenue Jean Nicoli, 20250 Corte, France\n",
      "\n",
      "---\n",
      "\n",
      "1\n",
      "Introduction\n",
      "\n",
      "---\n",
      "\n",
      "1.1\n",
      "In a nutshell\n",
      "\n",
      "---\n",
      "\n",
      "We prove here the consistency of NPHC estimator using the framework of Generalized Method of Moments Hansen\n",
      "[1982]. The main difference with the usual Generalized Method of Moments relies in the relaxation of the moment\n",
      "conditions, since we have E[�gT (θ0)] = mT ̸= 0. We adapt the proof of consistency given in Newey and McFadden\n",
      "[1994].\n",
      "\n",
      "---\n",
      "\n",
      "1.2\n",
      "Sketch of the proof\n",
      "\n",
      "---\n",
      "\n",
      "We can relate the integral of the Hawkes process’s kernels to the integrals of the cumulant densities, from Jovanovi´c\n",
      "et al. [2015]. Our cumulant matching method would fall into the usual GMM framework if we could estimate -\n",
      "without bias - the integral of the covariance on R, and the integral of the skewness on R2. Unfortunately, we can’t do\n",
      "that easily. We can however estimate without bias\n",
      "�\n",
      "f T\n",
      "t Cij\n",
      "t dt and\n",
      "�\n",
      "f T\n",
      "t Kijk\n",
      "t\n",
      "dt with f T a compact supported function\n",
      "on [−HT , HT ] that weakly converges to 1, with HT −→\n",
      "T →∞ ∞. In most cases we will take f T\n",
      "t = 1[−HT ,HT ](t).\n",
      "\n",
      "---\n",
      "\n",
      "Denoting �Cij,(T ) the estimator of\n",
      "�\n",
      "f T\n",
      "t Cij\n",
      "t dt, the term |E[ �Cij,(T )] − Cij| = |\n",
      "�\n",
      "f T\n",
      "t Cij\n",
      "t dt − Cij| can be considered a\n",
      "proxy to the distance to the classical GMM. This distance has to go to zero to make the rest of GMM’s proof work:\n",
      "the estimator �Cij,(T ) is then asymptotically unbiased towards Cij when T goes to inﬁnity.\n",
      "\n",
      "---\n",
      "\n",
      "1.3\n",
      "Notations\n",
      "\n",
      "---\n",
      "\n",
      "We observe the multivariate point process (N t) on R+, with Zi the events of the ith component. We will often write\n",
      "covariance / skewness instead of integrated covariance / skewness. In the rest of the document, we use the following\n",
      "notations.\n",
      "\n",
      "---\n",
      "\n",
      "Hawkes kernels’ integrals\n",
      "Gtrue =\n",
      "�\n",
      "Φtdt = (\n",
      "�\n",
      "φij\n",
      "t dt)ij = Id − (Rtrue)−1\n",
      "\n",
      "---\n",
      "\n",
      "Theoretical mean matrix\n",
      "L = diag(Λ1, . . . , Λd)\n",
      "\n",
      "---\n",
      "\n",
      "Theoretical covariance\n",
      "C = RtrueL(Rtrue)⊤\n",
      "\n",
      "---\n",
      "\n",
      "∗massil.achab@m4x.org\n",
      "\n",
      "---\n",
      "\n",
      "1\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in page.get_text_blocks():\n",
    "    print(b[4])\n",
    "    print('---')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.37000274658203,\n",
       " 107.21038818359375,\n",
       " 532.6320190429688,\n",
       " 151.30178833007812,\n",
       " 'Supplementary material of the article Uncovering Causality from\\nMultivariate Hawkes Integrated Cumulants\\n',\n",
       " 0,\n",
       " 0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Synthesizing\n",
    "\n",
    "#Choose the highest probability word at each step until it chooses \\end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2(dump_folder = './scraped/'):\n",
    "    \n",
    "    # Part 1 - Scrape PDFs (if not already done)\n",
    "    if dump_folder[-1] != '/':\n",
    "        dump_folder += '/'\n",
    "    files = os.listdir(dump_folder)\n",
    "    if len(files) < 10:\n",
    "        #Probably haven't scraped\n",
    "        scrape(dump_folder)\n",
    "\n",
    "    # Part 2 - Load and process PDFs\n",
    "    for fp in os.listdir(dump_folder):\n",
    "        if fp.endswith('.pdf'): #Don't care about any supplementary files (log.txt)\n",
    "            #PDF to be loaded in\n",
    "            pass\n",
    "\n",
    "\n",
    "    # Part 3 - Synthesize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Jhanvi\n",
    "\n",
    "Continue building your toolbox on Kaggle. Work on submissions for the same competition\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "1. What is the best Kaggle forum post that you found? Briefly describe what you learned from\n",
    "it.\n",
    "2. What is the best public leader board (LB) score you can achieve? Describe your approach.\n",
    "3. Submit a model that is definitely overfitting and a model that is definitely underfitting.\n",
    "\n",
    "\n",
    "Overfitting means that your training error is much smaller compared to your test error (and LB score).   \n",
    "Underfitting means that your model is too simple and even the training error is very large (and so will the test error).  \n",
    "You can experiment with depth of decision trees in random forests or XGBoost classifiers as the metric of complexity for your models, or any other family of models you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
