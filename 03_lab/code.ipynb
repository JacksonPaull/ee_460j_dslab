{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Josh\n",
    "\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’.  \n",
    "Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication.\n",
    "https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "\n",
    "*Q: Summarize what you learned briefly (e.g. half a page).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<Summary\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Jackson  \n",
    "\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2017 papers from http://proceedings.mlr.press/v70/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "2. Let Zbe a randomly selected word in a randomly selected ICML paper. Estimate the entropy\n",
    "of Z.\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "4. (Extra credit) Synthesize a random paragraph using an n-gram model on words. Synthesize\n",
    "a random paragraph using any model you want. Top five synthesized text paragraphs win\n",
    "bonus (+30 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import logging\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "def scrape(dump_folder, source):\n",
    "    if dump_folder[-1] != '/':\n",
    "        dump_folder += '/'\n",
    "    #Create folder to dump into\n",
    "    if not os.path.isdir(dump_folder):\n",
    "        os.mkdir(dump_folder)\n",
    "\n",
    "    #Set up logging\n",
    "    f = open(f'{dump_folder}log.txt', 'w') #Open the file if its not already opened\n",
    "    f.close()\n",
    "    logging.basicConfig(level=logging.INFO, filename=f'{dump_folder}log.txt')\n",
    "    \n",
    "    #Get list of links\n",
    "    html = requests.get(source)\n",
    "    soup = bs(html.content, 'html.parser')\n",
    "    links = soup.findAll('a')\n",
    "\n",
    "    #Scrape all PDFs\n",
    "    names = []\n",
    "    for l in links:\n",
    "        if l.decode_contents() == 'Download PDF' or l.decode_contents() == 'Supplementary PDF':\n",
    "            src = l.get('href').replace('ı', 'i') # Fix small error in one of the scraped links\n",
    "            fname = src[src.rindex('/')+1:]\n",
    "            if fname in names:\n",
    "                logging.CRITICAL(f'OVERWRITING FILE WITH NAME {fname}')\n",
    "            names.append(fname)\n",
    "            logging.info(f'Scraping pdf from {src} into {fname}')\n",
    "\n",
    "            pdf = requests.get(src)\n",
    "            with open(f'{dump_folder}{fname}', 'wb') as f:\n",
    "                f.write(pdf.content)\n",
    "\n",
    "def parse_pdf(fp):\n",
    "    df = pd.DataFrame(columns=['word', 'prev'])\n",
    "    pdf = fitz.open(fp)\n",
    "    for page in pdf:\n",
    "        _blocks = pdf[0].get_text_blocks()\n",
    "        blocks = []\n",
    "        for b in _blocks:\n",
    "            if b[-1] != 1:\n",
    "                blocks.append(b)\n",
    "\n",
    "        #Create 2D array of words by block\n",
    "        data = [x[4].replace('. ', ' . ').split() +['\\end'] for x in blocks]\n",
    "        if data == []:\n",
    "            continue\n",
    "        mlen = max([len(d) for d in data])\n",
    "        data = [d + ['' for i in range(mlen - len(d) + 1)] for d in data]\n",
    "\n",
    "        #Create array of previous words by block\n",
    "        words = np.array(data)\n",
    "        prev = np.roll(words, 1, axis=1)\n",
    "        prev[:, 0] = '\\start'\n",
    "\n",
    "        words = words.flatten()\n",
    "        prev = prev.flatten()\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(data = np.column_stack([words, prev]), columns=['word', 'prev'])])\n",
    "\n",
    "    return df.drop(df[(df['word']=='') | (df['prev']=='')].index)\n",
    "\n",
    "\n",
    "def parse_pdfs(dump_folder):\n",
    "    df = pd.DataFrame(columns=['word', 'prev'])\n",
    "    for fp in tqdm(os.listdir(dump_folder)):\n",
    "        if not fp.endswith('pdf'):\n",
    "            continue #Only process PDFs\n",
    "        df = pd.concat([df, parse_pdf(dump_folder+fp)])\n",
    "\n",
    "    df['count'] = 1\n",
    "    df = pd.pivot_table(df, values='count', index=['prev', 'word'], aggfunc=np.sum)\n",
    "    df = df.reset_index(level=1)\n",
    "    df = df[(df.index.str.match(r'^[\\\\a-zA-Z\\.]+$')) & (df['word'].str.match(r'^[\\\\a-zA-Z\\.]+$'))]\n",
    "    df['total'] = df.groupby(by='prev').transform('sum')['count'].astype(int)\n",
    "    df['percent'] = df['count'] / df['total']\n",
    "    df['cumsum'] = df.groupby(by='prev').transform('cumsum')['percent'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def predict(df, prev_word):\n",
    "    \"\"\"\n",
    "    Given a previous word, and a marginal distribution dataframe, predict the word that follows it\n",
    "    \"\"\"\n",
    "    p = random()\n",
    "    words = df.loc[prev_word]\n",
    "    if type(words) == pd.Series:\n",
    "        return words['word']\n",
    "    words = words[words['cumsum'] > p]\n",
    "    return words.iloc[0]['word']\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def synthesize_text(use_stored_data= True, df_fp='./distir.csv', save=True, source='http://proceedings.mlr.press/v70/', dump_folder = './scraped/', scrape_data=True):\n",
    "    \"\"\"If stored data exists, use it\n",
    "    Otherwise scrape and process\n",
    "    \n",
    "\n",
    "    Then iteratively predict what word comes next\"\"\"\n",
    "\n",
    "    # Scrape if necessary\n",
    "    if not (use_stored_data and os.path.exists(df_fp)):\n",
    "        if scrape_data:\n",
    "            print('Scraping...', end='')\n",
    "            scrape(dump_folder, source)\n",
    "            print('Done!')\n",
    "        df = parse_pdfs(dump_folder)\n",
    "        \n",
    "        #Save if wanted\n",
    "        if save:\n",
    "            df.to_csv(df_fp)\n",
    "    else:\n",
    "        df = pd.read_csv(df_fp, index_col=0)\n",
    "\n",
    "    # Iteratively Synthesize Text\n",
    "    words = ['\\start']\n",
    "    while words[-1] != '\\end':\n",
    "        words += [predict(df, words[-1])]\n",
    "        \n",
    "    return (' '.join(words[1:-1])).replace(' . ', '. ')\n",
    "\n",
    "def top_words(df, n=10):\n",
    "    return list(df.groupby(by='word')['count'].sum().sort_values(ascending=False).head(n).index)\n",
    "\n",
    "def entropy(_df, word=None):\n",
    "    if word is None:\n",
    "        word = _df['word'].sample(1).values[0]\n",
    "\n",
    "    df = _df[_df['word'] == word]\n",
    "\n",
    "    return word, -1 * (np.sum(df['percent'] * np.log(df['percent'])/np.log(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', '.', 'a', 'to', '\\\\end', 'is', 'and', 'in', 'that']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 words\n",
    "df = pd.read_csv('./distir.csv')\n",
    "top_words(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('implement', 0.1260897249795341)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z is still not have too sensitive to quadratically in the learnable embeddings separately by a good prediction As its evolution over binary vectors. These include the amount of the system. Exact solutions to the full history and useful and was naturally generate sparse generalized WNN by combining the neural architectures despite its performance of Thm. We note that leads to estimate the feasible region P. . Fix a typical case of the c.d.f. Correspondence'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways and synthesized text\n",
    "\n",
    "> C \\\\\\\\ S is the input perturbation of about individuals\n",
    "\n",
    "> Z is still not have too sensitive to quadratically in the learnable embeddings separately by a good prediction As its evolution over binary vectors. These include the amount of the system. Exact solutions to the full history and useful and was naturally generate sparse generalized WNN by combining the neural architectures despite its performance of Thm. We note that leads to estimate the feasible region P. . Fix a typical case of the c.d.f. Correspondence\n",
    "\n",
    "Above are examples of text that was produced using only the marginal distribution of words seen in the scraped PDFs, however it is not a very good approximation of human sentences.\n",
    "This is due to the fact that words are not constructed in the context of the previously seen word, but rather all words in the sentence and paragraph prior to it.\n",
    "\n",
    "An n-gram model takes the same concept but expands to be trained on the context of the n-previous words, with a higher n leading to much better text generation.\n",
    "This still has its limitations however, as even a very large corpus does not have enough data to fully capture the range of possible human speech, so recurrent neural networks trained on tokenized text is a better next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Jhanvi\n",
    "\n",
    "Continue building your toolbox on Kaggle. Work on submissions for the same competition\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "1. What is the best Kaggle forum post that you found? Briefly describe what you learned from\n",
    "it.\n",
    "2. What is the best public leader board (LB) score you can achieve? Describe your approach.\n",
    "3. Submit a model that is definitely overfitting and a model that is definitely underfitting.\n",
    "\n",
    "\n",
    "Overfitting means that your training error is much smaller compared to your test error (and LB score).   \n",
    "Underfitting means that your model is too simple and even the training error is very large (and so will the test error).  \n",
    "You can experiment with depth of decision trees in random forests or XGBoost classifiers as the metric of complexity for your models, or any other family of models you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
