{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Josh\n",
    "\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’.  \n",
    "Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication.\n",
    "https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "\n",
    "*Q: Summarize what you learned briefly (e.g. half a page).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<Summary\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Jackson  \n",
    "\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2017 papers from http://proceedings.mlr.press/v70/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "2. Let Zbe a randomly selected word in a randomly selected ICML paper. Estimate the entropy\n",
    "of Z.\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "4. (Extra credit) Synthesize a random paragraph using an n-gram model on words. Synthesize\n",
    "a random paragraph using any model you want. Top five synthesized text paragraphs win\n",
    "bonus (+30 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def scrape(dump_folder, source):\n",
    "    #Create folder to dump into\n",
    "    if not os.path.isdir(dump_folder):\n",
    "        os.mkdir(dump_folder)\n",
    "\n",
    "    #Set up logging\n",
    "    f = open(f'{dump_folder}log.txt', 'w') #Open the file if its not already opened\n",
    "    f.close()\n",
    "    logging.basicConfig(level=logging.INFO, filename=f'{dump_folder}log.txt')\n",
    "    \n",
    "    #Get list of links\n",
    "    html = requests.get(source)\n",
    "    soup = bs(html.content, 'html.parser')\n",
    "    links = soup.findAll('a')\n",
    "\n",
    "    #Scrape all PDFs\n",
    "    names = []\n",
    "    for l in links:\n",
    "        if l.decode_contents() == 'Download PDF' or l.decode_contents() == 'Supplementary PDF':\n",
    "            src = l.get('href').replace('ı', 'i') # Fix small error in one of the scraped links\n",
    "            fname = src[src.rindex('/')+1:]\n",
    "            if fname in names:\n",
    "                logging.CRITICAL(f'OVERWRITING FILE WITH NAME {fname}')\n",
    "            names.append(fname)\n",
    "            logging.info(f'Scraping pdf from {src} into {fname}')\n",
    "\n",
    "            pdf = requests.get(src)\n",
    "            with open(f'{dump_folder}{fname}', 'wb') as f:\n",
    "                f.write(pdf.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_pdf(fp, df):\n",
    "    pdf = fitz.open(fp)\n",
    "    for page in pdf:\n",
    "        blocks = page.get_text_blocks()\n",
    "        for block in blocks:\n",
    "            if block[-1] == 1:\n",
    "                continue #This is an image, do not parse\n",
    "            \n",
    "            words = block[4].replace('.', ' .').split()\n",
    "            for i, word in enumerate(words):\n",
    "                if i == 0:\n",
    "                    prev = '\\\\start'\n",
    "                else:\n",
    "                    prev = words[i-1]\n",
    "                \n",
    "                locator = (df['word'] == word) & (df['prev'] == prev)\n",
    "                if not (locator).any():\n",
    "                    data = dict(word=word, prev=prev, count=1)\n",
    "                    df = df.append(data, ignore_index=True)\n",
    "                else:\n",
    "                    df.loc[locator, 'count'] += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_pdfs(dump_folder):\n",
    "    df = pd.DataFrame(columns=['word', 'prev', 'count'])\n",
    "\n",
    "    for fp in tqdm(os.listdir(dump_folder)):\n",
    "        if not fp.endswith('pdf'):\n",
    "            continue #Only process PDFs\n",
    "\n",
    "        df = parse_pdf(dump_folder+fp, df)\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "pdf = fitz.open('./scraped/achab17a-supp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "_blocks = pdf[0].get_text_blocks()\n",
    "blocks = []\n",
    "for b in _blocks:\n",
    "    if b[-1] != 1:\n",
    "        blocks.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x[4].replace('. ', ' . ').split() +['\\end'] for x in blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlen = max([len(d) for d in data])\n",
    "data = [d + ['' for i in range(mlen - len(d) + 1)] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(data)\n",
    "prev = np.roll(words, 1, axis=1)\n",
    "prev[:, 0] = '\\start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.flatten()\n",
    "prev = prev.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([words, prev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = x, columns=['word', 'prev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supplementary</td>\n",
       "      <td>material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article</td>\n",
       "      <td>Uncovering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Causality</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Hawkes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word        prev\n",
       "0  Supplementary    material\n",
       "1             of         the\n",
       "2        article  Uncovering\n",
       "3      Causality        from\n",
       "4   Multivariate      Hawkes"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = parse_pdfs(dump_folder='./scraped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Synthesizing\n",
    "\n",
    "#Choose the highest probability word at each step until it chooses \\end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2(source='http://proceedings.mlr.press/v70/', \n",
    "                dump_folder = './scraped/'):\n",
    "    \n",
    "    # Part 1 - Scrape PDFs (if not already done)\n",
    "    if dump_folder[-1] != '/':\n",
    "        dump_folder += '/'\n",
    "    files = os.listdir(dump_folder)\n",
    "    if len(files) < 10:\n",
    "        #Probably haven't scraped\n",
    "        scrape(dump_folder, source)\n",
    "\n",
    "    # Part 2 - Load and process PDFs\n",
    "    df = parse_pdfs(dump_folder)\n",
    "\n",
    "\n",
    "    # Part 3 - Synthesize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Jhanvi\n",
    "\n",
    "Continue building your toolbox on Kaggle. Work on submissions for the same competition\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "1. What is the best Kaggle forum post that you found? Briefly describe what you learned from\n",
    "it.\n",
    "2. What is the best public leader board (LB) score you can achieve? Describe your approach.\n",
    "3. Submit a model that is definitely overfitting and a model that is definitely underfitting.\n",
    "\n",
    "\n",
    "Overfitting means that your training error is much smaller compared to your test error (and LB score).   \n",
    "Underfitting means that your model is too simple and even the training error is very large (and so will the test error).  \n",
    "You can experiment with depth of decision trees in random forests or XGBoost classifiers as the metric of complexity for your models, or any other family of models you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b328994b5f3347d233e8e3c9aa119482ce1b63da6676fdb53c0b7e84e61721bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
