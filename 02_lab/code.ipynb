{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 1: Covariance - Josh\n",
    "- When given a data matrix, an easy way to tell if any two columns are correlated is to look at a scatter plot of each column against each other column.  \n",
    "For a warm up, do this:  \n",
    "Look at the data in DF1 in Lab2 Data.zip.  \n",
    "Which columns are (pairwise) correlated?  \n",
    "Figure out how to do this with Pandas, and also how to do this with Seaborn.  \n",
    "\n",
    "- Compute the covariance matrix of the data. Write the explicit expression for what this is,  \n",
    "and then use any command you like (e.g., np.cov) to compute the 4 ×4 matrix.  \n",
    "Explain why the numbers that you get fit with the plots you got.\n",
    "\n",
    "- The above problem in reverse. Generate a zero-mean multivariate Gaussian random variable in 3 dimensions,  \n",
    "Z = (X1,X2,X3) so that (X1,X2) and (X1,X3) are uncorrelated, but (X2,X3) are correlated.  \n",
    "Specifically: choose a covariance matrix that has the above correlations structure, and write this down. Then find a way to generate samples from this Gaussian.  \n",
    "Choose one of the non-zero covariance terms (Cij , if C denotes your covariance matrix) and plot it vs the estimated covariance term, as the number of samples you use scales.  \n",
    "The goal is to get a visual representation of how the empirical covariance converges to the true (or family) covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 2: Outliers - Jackson\n",
    "\n",
    "Consider the two-dimensional data in DF2 in *Lab2 Data.zip*.  \n",
    "<b>Look at a scatter plot of the data.</b>  \n",
    "\n",
    "It contains two points that look like potential outliers.  \n",
    "<b>Which one is “more” outlying?</b>  \n",
    "Propose a transformation of the data that makes it clear that the point at (−1,1) is more outlying than the point at (5.5,5),   \n",
    "even though the latter point is “farther away” from the nearest points.  \n",
    "Plot the data again after performing this transformation.  \n",
    "Provide discussion as appropriate to justify your choice of transformation.  \n",
    "<i>Hint: if y comes from a standard Gaussian in two dimensions  \n",
    "(i.e., with covariance equal to the two by two identity matrix)</i>\n",
    "\n",
    "$$Q = \\begin{bmatrix} 2 & 1/2 \\\\ 1/2 & 2\\end{bmatrix}$$\n",
    "\n",
    "What is the covariance matrix of the random variable z = Qy? If you are given z, how would you\n",
    "create a random Gaussian vector with covariance equal to the identity, using z?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 3: Popular Names - Jhanvi\n",
    "\n",
    "The goal of this exercise is for you to get more experience with Pandas, and to get a chance to  \n",
    "explore a cool data set. Download the file Names.zip from Canvas. This contains the frequency  \n",
    "of all names that appeared more than 5 times on a social security application from 1880 through 2015.\n",
    "- Write a program that on input k and XXXX, returns the top k names from year XXXX.  \n",
    "- Write a program that on input Name returns the frequency for men and women of the name `Name`.\n",
    "- It could be that names are more diverse now than they were in 1880, so that a name may be relatively the most popular, though its frequency may have been decreasing over the years.  \n",
    "Modify the above to return the relative frequency. Note that in the next coming lectures we will learn how to quantify diversity using entropy.\n",
    "- Find all the names that used to be more popular for one gender, but then became more popular for another gender.\n",
    "- (Optional) Find something cool about this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = pd.read_csv('Names.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 4: Starting in Kaggle - Josh\n",
    "\n",
    "Later in this class, you will be participating in the in-class Kaggle competition made specifically\n",
    "for this class. In that one, you will be participating on your own. This is a warmup- the more\n",
    "effort and research you put into this assignment the easier it will be to compete into the real Kaggle\n",
    "competition that you will need to do soon.\n",
    "1. Let’s start with our first Kaggle submission in a playground regression competition. Make an\n",
    "account to Kaggle and find\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "2. Follow the data preprocessing steps from\n",
    "https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-\n",
    "models. Then run a ridge regression using α = 0.1. Make a submission of this prediction,\n",
    "what is the RMSE you get?\n",
    "(Hint: remember to exponentiate np.expm1(ypred) your predictions).\n",
    "3. Compare a ridge regression and a lasso regression model. Optimize the alphas using cross\n",
    "validation. What is the best score you can get from a single ridge regression model and from\n",
    "a single lasso model?\n",
    "4. Plot the l0 norm (number of nonzeros) of the coefficients that lasso produces as you vary the\n",
    "strength of regularization parameter alpha.\n",
    "5. Add the outputs of your models as features and train a ridge regression on all the features\n",
    "plus the model outputs (This is called Ensembling and Stacking). Be careful not to overfit.\n",
    "What score can you get? (We will be discussing ensembling more, later in the class, but you\n",
    "can start playing with it now).\n",
    "6. Install XGBoost (Gradient Boosting) and train a gradient boosting regression. What score\n",
    "can you get just from a single XGB? (you will need to optimize over its parameters). We will\n",
    "discuss boosting and gradient boosting in more detail later. XGB is a great friend to all good\n",
    "Kagglers!\n",
    "7. Do your best to get the more accurate model. Try feature engineering and stacking many\n",
    "models. You are allowed to use any public tool in python. No non-python tools allowed.\n",
    "8. (Optional) Read the Kaggle forums, tutorials and Kernels in this competition. This is an\n",
    "excellent way to learn. Include in your report if you find something in the forums you like, or\n",
    "if you made your own post or code post, especially if other Kagglers liked or used it afterwards.\n",
    "2\n",
    "9. Be sure to read and learn the rules of Kaggle! No sharing of code or data outside the Kaggle\n",
    "forums. Every student should have their own individual Kaggle account and teams can be\n",
    "formed in the Kaggle submissions with partners. This is more important for live competitions\n",
    "of course.\n",
    "10. As in the real in-class Kaggle competition (which will be next), you will be graded based on\n",
    "your public score (include that in your report) and also on the creativity of your solution.\n",
    "In your report (that you will submit as a pdf file), explain what worked and what did\n",
    "not work. Many creative things will not work, but you will get partial credit for developing\n",
    "them. We will invite teams with interesting solutions to present them in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}